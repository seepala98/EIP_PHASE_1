{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment_test_initial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHz7IRhZKxNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4baf9cd4-3606-45b5-f542-d4c5868fda40"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "import keras\n",
        "from keras.applications import VGG16, InceptionV3\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model, load_model, save_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,LearningRateScheduler, EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "from IPython.core.display import display\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHsPYyhRIBg",
        "colab_type": "code",
        "outputId": "464741b3-e1e2-4eeb-aab5-d376cc892a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "drive_mount = os.path.abspath('/content/gdrive')\n",
        "base_dir = os.path.join(drive_mount, 'My Drive/human_data')\n",
        "models_dir = os.path.join(base_dir, 'saved_models')\n",
        "\n",
        "\n",
        "# mount gdrive and unzip data\n",
        "\n",
        "drive.mount(drive_mount, force_remount=True)\n",
        "!unzip -qo \"./gdrive/My Drive/human_data/hvc_data.zip\"\n",
        "print(os.listdir(base_dir))\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "['hvc_data.zip', 'saved_models']\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKfybYYSLkzx",
        "colab_type": "code",
        "outputId": "e20762d4-4b91-4482-85e4-24f7a6bb573e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "display(df)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13568</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Happy</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13570.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13569</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>25-35</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Fancy</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13571.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13570</th>\n",
              "      <td>female</td>\n",
              "      <td>Bad</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Side</td>\n",
              "      <td>resized/13572.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13571</th>\n",
              "      <td>female</td>\n",
              "      <td>Bad</td>\n",
              "      <td>25-35</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13573.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13572</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>25-35</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Happy</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/13574.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13573 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       gender imagequality  ...        bodypose         image_path\n",
              "0        male      Average  ...  Front-Frontish      resized/1.jpg\n",
              "1      female      Average  ...  Front-Frontish      resized/2.jpg\n",
              "2        male         Good  ...  Front-Frontish      resized/3.jpg\n",
              "3        male         Good  ...  Front-Frontish      resized/4.jpg\n",
              "4      female         Good  ...  Front-Frontish      resized/5.jpg\n",
              "...       ...          ...  ...             ...                ...\n",
              "13568    male      Average  ...  Front-Frontish  resized/13570.jpg\n",
              "13569  female      Average  ...  Front-Frontish  resized/13571.jpg\n",
              "13570  female          Bad  ...            Side  resized/13572.jpg\n",
              "13571  female          Bad  ...  Front-Frontish  resized/13573.jpg\n",
              "13572    male         Good  ...  Front-Frontish  resized/13574.jpg\n",
              "\n",
              "[13573 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpvaSjOpLmxN",
        "colab_type": "code",
        "outputId": "4da41368-1d58-409a-c3b8-61e1ddae0dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat(\n",
        "    [\n",
        "        df[[\"image_path\"]],\n",
        "        pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "        pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "        pd.get_dummies(df.age, prefix=\"age\"),\n",
        "        pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "        pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "        pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "        pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "        pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "    ], \n",
        "    axis = 1\n",
        ")\n",
        "\n",
        "display(one_hot_df)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13568</th>\n",
              "      <td>resized/13570.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13569</th>\n",
              "      <td>resized/13571.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13570</th>\n",
              "      <td>resized/13572.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13571</th>\n",
              "      <td>resized/13573.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13572</th>\n",
              "      <td>resized/13574.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13573 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0          resized/1.jpg              0  ...                        1              0\n",
              "1          resized/2.jpg              1  ...                        1              0\n",
              "2          resized/3.jpg              0  ...                        1              0\n",
              "3          resized/4.jpg              0  ...                        1              0\n",
              "4          resized/5.jpg              1  ...                        1              0\n",
              "...                  ...            ...  ...                      ...            ...\n",
              "13568  resized/13570.jpg              0  ...                        1              0\n",
              "13569  resized/13571.jpg              1  ...                        1              0\n",
              "13570  resized/13572.jpg              1  ...                        0              1\n",
              "13571  resized/13573.jpg              1  ...                        1              0\n",
              "13572  resized/13574.jpg              0  ...                        1              0\n",
              "\n",
              "[13573 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw8UO65xLpJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM-d3lULOGt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_list = [_gender_cols_, _imagequality_cols_, _age_cols_, _weight_cols_, _carryingbag_cols_, _footwear_cols_, _emotion_cols_, _bodypose_cols_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LevsjOOgOJBr",
        "colab_type": "code",
        "outputId": "582a08d0-d55d-4816-e2c0-07c7909ae838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for entity in (col_list):\n",
        "  print(entity)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gender_female', 'gender_male']\n",
            "['imagequality_Average', 'imagequality_Bad', 'imagequality_Good']\n",
            "['age_15-25', 'age_25-35', 'age_35-45', 'age_45-55', 'age_55+']\n",
            "['weight_normal-healthy', 'weight_over-weight', 'weight_slightly-overweight', 'weight_underweight']\n",
            "['carryingbag_Daily/Office/Work Bag', 'carryingbag_Grocery/Home/Plastic Bag', 'carryingbag_None']\n",
            "['footwear_CantSee', 'footwear_Fancy', 'footwear_Normal']\n",
            "['emotion_Angry/Serious', 'emotion_Happy', 'emotion_Neutral', 'emotion_Sad']\n",
            "['bodypose_Back', 'bodypose_Front-Frontish', 'bodypose_Side']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UpWzG9NLtjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])    \n",
        "        \n",
        "        if self.augmentation:\n",
        "          eraser = get_random_eraser()\n",
        "          aug_image_stack = np.stack([eraser(img) for img in images])    \n",
        "\n",
        "        # if self.augmentation is not None:\n",
        "        #     images = self.augmentation.flow(images, shuffle=False).next()\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "\n",
        "        if self.augmentation:\n",
        "          return aug_image_stack, target\n",
        "\n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK9aWYXOL2oj",
        "colab_type": "code",
        "outputId": "6fb548b1-4d44-4f33-8e52-bc625fc7a211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "\n",
        "print(train_df.shape, val_df.shape)\n",
        "display(train_df)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11537, 28) (2036, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9257</th>\n",
              "      <td>resized/9258.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2661</th>\n",
              "      <td>resized/2662.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7031</th>\n",
              "      <td>resized/7032.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12162</th>\n",
              "      <td>resized/12164.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>resized/1285.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2694</th>\n",
              "      <td>resized/2695.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3724</th>\n",
              "      <td>resized/3725.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10971</th>\n",
              "      <td>resized/10973.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558</th>\n",
              "      <td>resized/1559.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10251</th>\n",
              "      <td>resized/10253.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11537 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "9257    resized/9258.jpg              1  ...                        1              0\n",
              "2661    resized/2662.jpg              1  ...                        1              0\n",
              "7031    resized/7032.jpg              0  ...                        0              0\n",
              "12162  resized/12164.jpg              0  ...                        1              0\n",
              "1284    resized/1285.jpg              1  ...                        1              0\n",
              "...                  ...            ...  ...                      ...            ...\n",
              "2694    resized/2695.jpg              1  ...                        0              1\n",
              "3724    resized/3725.jpg              0  ...                        1              0\n",
              "10971  resized/10973.jpg              0  ...                        1              0\n",
              "1558    resized/1559.jpg              0  ...                        0              1\n",
              "10251  resized/10253.jpg              0  ...                        1              0\n",
              "\n",
              "[11537 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lHncbBTQtGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, _ = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        c = np.random.uniform(v_l, v_h)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltorEq5Wg2iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWQ7_UA6L5Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation=datagen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False,augmentation=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j-NXZ3-MDdi",
        "colab_type": "code",
        "outputId": "d35858ee-3ec5-46c0-9388-bcbd5787b545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "print('no. of units:', num_units)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of units: {'gender': 2, 'image_quality': 3, 'age': 5, 'weight': 4, 'bag': 3, 'pose': 3, 'footwear': 3, 'emotion': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI5q84FHMLqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "4738f48d-fa30-4a9b-85df-0f282c9057e0"
      },
      "source": [
        "backbone = VGG16(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = BatchNormalization()(neck)\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(256, activation=\"relu\")(neck)\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.05)(in_layer)\n",
        "    neck = BatchNormalization()(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.1)(neck)\n",
        "    neck = BatchNormalization()(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\")(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnLz8vtTMzlF",
        "colab_type": "code",
        "outputId": "75d61ca7-9bc6-4ebc-876e-036ad215dc61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 7, 7, 512)    2048        block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 25088)        0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          6422784     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 256)          1024        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 256)          1024        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 256)          1024        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 256)          1024        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 256)          1024        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 256)          1024        dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 256)          1024        dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 256)          1024        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          32896       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          32896       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          32896       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 128)          32896       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          32896       batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          32896       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 128)          32896       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 128)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 128)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 128)          0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 128)          0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 128)          0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128)          512         dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 128)          512         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 128)          512         dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 128)          512         dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 128)          512         dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 128)          512         dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 128)          512         dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 128)          512         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          16512       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          16512       batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          16512       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          16512       batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 128)          16512       batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 128)          16512       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 128)          16512       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 128)          16512       batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 21,550,555\n",
            "Trainable params: 21,543,387\n",
            "Non-trainable params: 7,168\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sclCgE1bfDra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feyt-vc6fF46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7R__RJxM2AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fez5o3S6M8rO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1df11b29-985c-4769-8656-4bba33f9b2f7"
      },
      "source": [
        "\n",
        "losses = {\n",
        "\t\"gender_output\": \"binary_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.3, \"weight_output\":1.0,\"bag_output\":1.0,\"footwear_output\":1.0,\"pose_output\":1.0,\"emotion_output\":1.0}\n",
        "opt = SGD(lr=1e-3, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss= losses,\n",
        "    #loss=\"categorical_crossentropy\", \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrmargvCQI8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 45:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 35:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 25:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 15:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y8Bd-BJQYwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def update_callbacks():\n",
        "\n",
        "#   # Prepare model model saving directory.\n",
        "#   # save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "#   save_dir = F\"/content/gdrive/My Drive/saved_models\" \n",
        "#   model_name = 'Wk5_model.{epoch:03d}.{val_acc:.4f}.h5'\n",
        "#   if not os.path.isdir(save_dir):\n",
        "#       os.makedirs(save_dir)\n",
        "#   filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "#   # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "#   checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                               monitor='val_acc',\n",
        "#                               verbose=1,\n",
        "#                               save_best_only=True)\n",
        "\n",
        "#   lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#   lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                 cooldown=0,\n",
        "#                                 patience=5,\n",
        "#                                 min_lr=0.5e-6)\n",
        "\n",
        "#   #callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "#   callbacks = [lr_reducer,lr_scheduler]\n",
        "\n",
        "#   return callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoGrIdOkfMt8",
        "colab_type": "code",
        "outputId": "773101d9-c851-430e-a9f2-6239923c8438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import datetime\n",
        "# Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "t = datetime.datetime.today()\n",
        "print(str(datetime.datetime.today()))\n",
        "save_dir = \"/content/gdrive/My Drive/Colab Notebooks/Assignment5/saved_models/\"+str(t)\n",
        "model_name = 'A5_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "filepath"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-29 15:45:25.128006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/Assignment5/saved_models/2019-12-29 15:45:25.127965/A5_model.{epoch:03d}.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dug4Tuf5fL2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "''\n",
        "\n",
        "#es = EarlyStopping(monitor='val_loss', verbose=1, patience=5, mode='min')\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint,lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9awGA8oNKY7",
        "colab_type": "code",
        "outputId": "9c16a946-313e-4eb7-8947-407e464048de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    callbacks = callbacks,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.1577 - gender_output_loss: 0.5158 - image_quality_output_loss: 0.9922 - age_output_loss: 1.5090 - weight_output_loss: 1.0981 - bag_output_loss: 0.9634 - footwear_output_loss: 0.8675 - pose_output_loss: 0.7659 - emotion_output_loss: 0.9930 - gender_output_acc: 0.7478 - image_quality_output_acc: 0.5245 - age_output_acc: 0.3561 - weight_output_acc: 0.5805 - bag_output_acc: 0.5540 - footwear_output_acc: 0.6083 - pose_output_acc: 0.6704 - emotion_output_acc: 0.6775\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 8.1582 - gender_output_loss: 0.5158 - image_quality_output_loss: 0.9920 - age_output_loss: 1.5100 - weight_output_loss: 1.0985 - bag_output_loss: 0.9635 - footwear_output_loss: 0.8673 - pose_output_loss: 0.7655 - emotion_output_loss: 0.9926 - gender_output_acc: 0.7479 - image_quality_output_acc: 0.5246 - age_output_acc: 0.3558 - weight_output_acc: 0.5802 - bag_output_acc: 0.5541 - footwear_output_acc: 0.6082 - pose_output_acc: 0.6708 - emotion_output_acc: 0.6779 - val_loss: 7.4500 - val_gender_output_loss: 0.4060 - val_image_quality_output_loss: 0.8998 - val_age_output_loss: 1.4325 - val_weight_output_loss: 1.0264 - val_bag_output_loss: 0.9063 - val_footwear_output_loss: 0.7978 - val_pose_output_loss: 0.6008 - val_emotion_output_loss: 0.9506 - val_gender_output_acc: 0.8160 - val_image_quality_output_acc: 0.5766 - val_age_output_acc: 0.3861 - val_weight_output_acc: 0.6159 - val_bag_output_acc: 0.5968 - val_footwear_output_acc: 0.6331 - val_pose_output_acc: 0.7429 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.45002, saving model to /content/gdrive/My Drive/Colab Notebooks/Assignment5/saved_models/2019-12-29 15:45:25.127965/A5_model.001.h5\n",
            "Epoch 2/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 6.9447 - gender_output_loss: 0.4119 - image_quality_output_loss: 0.8770 - age_output_loss: 1.3261 - weight_output_loss: 0.9286 - bag_output_loss: 0.8185 - footwear_output_loss: 0.7620 - pose_output_loss: 0.5704 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8069 - image_quality_output_acc: 0.5799 - age_output_acc: 0.4240 - weight_output_acc: 0.6371 - bag_output_acc: 0.6328 - footwear_output_acc: 0.6617 - pose_output_acc: 0.7674 - emotion_output_acc: 0.7132 - val_loss: 7.2633 - val_gender_output_loss: 0.3823 - val_image_quality_output_loss: 0.8952 - val_age_output_loss: 1.4178 - val_weight_output_loss: 1.0060 - val_bag_output_loss: 0.8781 - val_footwear_output_loss: 0.7704 - val_pose_output_loss: 0.5496 - val_emotion_output_loss: 0.9385 - val_gender_output_acc: 0.8322 - val_image_quality_output_acc: 0.5796 - val_age_output_acc: 0.3816 - val_weight_output_acc: 0.6154 - val_bag_output_acc: 0.6053 - val_footwear_output_acc: 0.6562 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6935\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.45002 to 7.26327, saving model to /content/gdrive/My Drive/Colab Notebooks/Assignment5/saved_models/2019-12-29 15:45:25.127965/A5_model.002.h5\n",
            "Epoch 3/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.4749 - gender_output_loss: 0.3575 - image_quality_output_loss: 0.8324 - age_output_loss: 1.2428 - weight_output_loss: 0.8780 - bag_output_loss: 0.7700 - footwear_output_loss: 0.7090 - pose_output_loss: 0.4956 - emotion_output_loss: 0.8168 - gender_output_acc: 0.8410 - image_quality_output_acc: 0.6073 - age_output_acc: 0.4634 - weight_output_acc: 0.6561 - bag_output_acc: 0.6549 - footwear_output_acc: 0.6924 - pose_output_acc: 0.7974 - emotion_output_acc: 0.7176\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 6.4764 - gender_output_loss: 0.3582 - image_quality_output_loss: 0.8330 - age_output_loss: 1.2428 - weight_output_loss: 0.8781 - bag_output_loss: 0.7703 - footwear_output_loss: 0.7094 - pose_output_loss: 0.4957 - emotion_output_loss: 0.8161 - gender_output_acc: 0.8409 - image_quality_output_acc: 0.6067 - age_output_acc: 0.4634 - weight_output_acc: 0.6559 - bag_output_acc: 0.6547 - footwear_output_acc: 0.6919 - pose_output_acc: 0.7973 - emotion_output_acc: 0.7179 - val_loss: 7.2435 - val_gender_output_loss: 0.3803 - val_image_quality_output_loss: 0.8936 - val_age_output_loss: 1.4239 - val_weight_output_loss: 0.9977 - val_bag_output_loss: 0.8761 - val_footwear_output_loss: 0.7629 - val_pose_output_loss: 0.5359 - val_emotion_output_loss: 0.9461 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.5796 - val_age_output_acc: 0.3851 - val_weight_output_acc: 0.6139 - val_bag_output_acc: 0.6154 - val_footwear_output_acc: 0.6663 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.26327 to 7.24353, saving model to /content/gdrive/My Drive/Colab Notebooks/Assignment5/saved_models/2019-12-29 15:45:25.127965/A5_model.003.h5\n",
            "Epoch 4/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 6.1055 - gender_output_loss: 0.3289 - image_quality_output_loss: 0.8000 - age_output_loss: 1.1779 - weight_output_loss: 0.8383 - bag_output_loss: 0.7209 - footwear_output_loss: 0.6698 - pose_output_loss: 0.4460 - emotion_output_loss: 0.7704 - gender_output_acc: 0.8579 - image_quality_output_acc: 0.6274 - age_output_acc: 0.5040 - weight_output_acc: 0.6677 - bag_output_acc: 0.6830 - footwear_output_acc: 0.7106 - pose_output_acc: 0.8204 - emotion_output_acc: 0.7234 - val_loss: 7.3107 - val_gender_output_loss: 0.3853 - val_image_quality_output_loss: 0.8887 - val_age_output_loss: 1.4367 - val_weight_output_loss: 1.0044 - val_bag_output_loss: 0.8871 - val_footwear_output_loss: 0.7713 - val_pose_output_loss: 0.5509 - val_emotion_output_loss: 0.9552 - val_gender_output_acc: 0.8337 - val_image_quality_output_acc: 0.5983 - val_age_output_acc: 0.3604 - val_weight_output_acc: 0.5993 - val_bag_output_acc: 0.6139 - val_footwear_output_acc: 0.6638 - val_pose_output_acc: 0.7807 - val_emotion_output_acc: 0.6709\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 7.24353\n",
            "Epoch 5/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 5.7671 - gender_output_loss: 0.3077 - image_quality_output_loss: 0.7643 - age_output_loss: 1.1048 - weight_output_loss: 0.7960 - bag_output_loss: 0.6759 - footwear_output_loss: 0.6341 - pose_output_loss: 0.4133 - emotion_output_loss: 0.7395 - gender_output_acc: 0.8658 - image_quality_output_acc: 0.6490 - age_output_acc: 0.5407 - weight_output_acc: 0.6809 - bag_output_acc: 0.7064 - footwear_output_acc: 0.7310 - pose_output_acc: 0.8391 - emotion_output_acc: 0.7340 - val_loss: 7.4488 - val_gender_output_loss: 0.3918 - val_image_quality_output_loss: 0.9120 - val_age_output_loss: 1.4578 - val_weight_output_loss: 1.0144 - val_bag_output_loss: 0.8942 - val_footwear_output_loss: 0.7935 - val_pose_output_loss: 0.5646 - val_emotion_output_loss: 0.9831 - val_gender_output_acc: 0.8296 - val_image_quality_output_acc: 0.5796 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6038 - val_bag_output_acc: 0.6134 - val_footwear_output_acc: 0.6623 - val_pose_output_acc: 0.7848 - val_emotion_output_acc: 0.6714\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.24353\n",
            "Epoch 6/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.3535 - gender_output_loss: 0.2758 - image_quality_output_loss: 0.7249 - age_output_loss: 1.0092 - weight_output_loss: 0.7383 - bag_output_loss: 0.6323 - footwear_output_loss: 0.5960 - pose_output_loss: 0.3747 - emotion_output_loss: 0.6995 - gender_output_acc: 0.8790 - image_quality_output_acc: 0.6728 - age_output_acc: 0.5947 - weight_output_acc: 0.7060 - bag_output_acc: 0.7325 - footwear_output_acc: 0.7484 - pose_output_acc: 0.8535 - emotion_output_acc: 0.7402\n",
            "Epoch 00005: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 5.3553 - gender_output_loss: 0.2758 - image_quality_output_loss: 0.7252 - age_output_loss: 1.0094 - weight_output_loss: 0.7383 - bag_output_loss: 0.6322 - footwear_output_loss: 0.5960 - pose_output_loss: 0.3759 - emotion_output_loss: 0.6997 - gender_output_acc: 0.8790 - image_quality_output_acc: 0.6724 - age_output_acc: 0.5946 - weight_output_acc: 0.7060 - bag_output_acc: 0.7326 - footwear_output_acc: 0.7483 - pose_output_acc: 0.8530 - emotion_output_acc: 0.7401 - val_loss: 7.6600 - val_gender_output_loss: 0.4025 - val_image_quality_output_loss: 0.9444 - val_age_output_loss: 1.5270 - val_weight_output_loss: 1.0314 - val_bag_output_loss: 0.9260 - val_footwear_output_loss: 0.7882 - val_pose_output_loss: 0.5866 - val_emotion_output_loss: 0.9958 - val_gender_output_acc: 0.8342 - val_image_quality_output_acc: 0.5817 - val_age_output_acc: 0.3498 - val_weight_output_acc: 0.6008 - val_bag_output_acc: 0.6084 - val_footwear_output_acc: 0.6578 - val_pose_output_acc: 0.7666 - val_emotion_output_acc: 0.6704\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.24353\n",
            "Epoch 7/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.0191 - gender_output_loss: 0.2612 - image_quality_output_loss: 0.6887 - age_output_loss: 0.9356 - weight_output_loss: 0.6949 - bag_output_loss: 0.5880 - footwear_output_loss: 0.5600 - pose_output_loss: 0.3481 - emotion_output_loss: 0.6619 - gender_output_acc: 0.8876 - image_quality_output_acc: 0.6909 - age_output_acc: 0.6305 - weight_output_acc: 0.7235 - bag_output_acc: 0.7513 - footwear_output_acc: 0.7664 - pose_output_acc: 0.8633 - emotion_output_acc: 0.7575\n",
            "Epoch 00006: val_loss did not improve from 7.24353\n",
            "Learning rate:  0.001\n",
            "\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 5.0212 - gender_output_loss: 0.2611 - image_quality_output_loss: 0.6888 - age_output_loss: 0.9362 - weight_output_loss: 0.6953 - bag_output_loss: 0.5884 - footwear_output_loss: 0.5603 - pose_output_loss: 0.3478 - emotion_output_loss: 0.6624 - gender_output_acc: 0.8878 - image_quality_output_acc: 0.6909 - age_output_acc: 0.6299 - weight_output_acc: 0.7233 - bag_output_acc: 0.7513 - footwear_output_acc: 0.7663 - pose_output_acc: 0.8635 - emotion_output_acc: 0.7571 - val_loss: 7.9569 - val_gender_output_loss: 0.4181 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.6144 - val_weight_output_loss: 1.0747 - val_bag_output_loss: 0.9434 - val_footwear_output_loss: 0.8280 - val_pose_output_loss: 0.5983 - val_emotion_output_loss: 1.0172 - val_gender_output_acc: 0.8342 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.3654 - val_weight_output_acc: 0.5781 - val_bag_output_acc: 0.5988 - val_footwear_output_acc: 0.6547 - val_pose_output_acc: 0.7727 - val_emotion_output_acc: 0.6497\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 7.24353\n",
            "Epoch 8/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.7226 - gender_output_loss: 0.2517 - image_quality_output_loss: 0.6509 - age_output_loss: 0.8671 - weight_output_loss: 0.6599 - bag_output_loss: 0.5568 - footwear_output_loss: 0.5285 - pose_output_loss: 0.3260 - emotion_output_loss: 0.6215 - gender_output_acc: 0.8908 - image_quality_output_acc: 0.7127 - age_output_acc: 0.6577 - weight_output_acc: 0.7429 - bag_output_acc: 0.7633 - footwear_output_acc: 0.7813 - pose_output_acc: 0.8726 - emotion_output_acc: 0.7739\n",
            "Epoch 00007: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 4.7220 - gender_output_loss: 0.2515 - image_quality_output_loss: 0.6513 - age_output_loss: 0.8668 - weight_output_loss: 0.6600 - bag_output_loss: 0.5567 - footwear_output_loss: 0.5280 - pose_output_loss: 0.3260 - emotion_output_loss: 0.6217 - gender_output_acc: 0.8909 - image_quality_output_acc: 0.7127 - age_output_acc: 0.6578 - weight_output_acc: 0.7428 - bag_output_acc: 0.7634 - footwear_output_acc: 0.7816 - pose_output_acc: 0.8724 - emotion_output_acc: 0.7737 - val_loss: 8.1963 - val_gender_output_loss: 0.4166 - val_image_quality_output_loss: 0.9942 - val_age_output_loss: 1.6866 - val_weight_output_loss: 1.1076 - val_bag_output_loss: 0.9621 - val_footwear_output_loss: 0.8538 - val_pose_output_loss: 0.6132 - val_emotion_output_loss: 1.0561 - val_gender_output_acc: 0.8311 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3634 - val_weight_output_acc: 0.5701 - val_bag_output_acc: 0.6139 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7802 - val_emotion_output_acc: 0.6573\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.24353\n",
            "Epoch 9/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.4432 - gender_output_loss: 0.2276 - image_quality_output_loss: 0.6222 - age_output_loss: 0.7984 - weight_output_loss: 0.6317 - bag_output_loss: 0.5176 - footwear_output_loss: 0.5077 - pose_output_loss: 0.3091 - emotion_output_loss: 0.5894 - gender_output_acc: 0.9044 - image_quality_output_acc: 0.7310 - age_output_acc: 0.6951 - weight_output_acc: 0.7505 - bag_output_acc: 0.7889 - footwear_output_acc: 0.7913 - pose_output_acc: 0.8800 - emotion_output_acc: 0.7846Learning rate:  0.001\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 4.4434 - gender_output_loss: 0.2275 - image_quality_output_loss: 0.6221 - age_output_loss: 0.7990 - weight_output_loss: 0.6318 - bag_output_loss: 0.5175 - footwear_output_loss: 0.5075 - pose_output_loss: 0.3093 - emotion_output_loss: 0.5891 - gender_output_acc: 0.9044 - image_quality_output_acc: 0.7311 - age_output_acc: 0.6949 - weight_output_acc: 0.7503 - bag_output_acc: 0.7889 - footwear_output_acc: 0.7915 - pose_output_acc: 0.8799 - emotion_output_acc: 0.7848 - val_loss: 8.5907 - val_gender_output_loss: 0.4462 - val_image_quality_output_loss: 1.0628 - val_age_output_loss: 1.7615 - val_weight_output_loss: 1.1430 - val_bag_output_loss: 1.0231 - val_footwear_output_loss: 0.8846 - val_pose_output_loss: 0.6391 - val_emotion_output_loss: 1.1019 - val_gender_output_acc: 0.8332 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3523 - val_weight_output_acc: 0.5832 - val_bag_output_acc: 0.6144 - val_footwear_output_acc: 0.6366 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6613\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 7.24353\n",
            "Epoch 10/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.1106 - gender_output_loss: 0.2258 - image_quality_output_loss: 0.5827 - age_output_loss: 0.7304 - weight_output_loss: 0.5635 - bag_output_loss: 0.4788 - footwear_output_loss: 0.4671 - pose_output_loss: 0.2789 - emotion_output_loss: 0.5642 - gender_output_acc: 0.9040 - image_quality_output_acc: 0.7469 - age_output_acc: 0.7167 - weight_output_acc: 0.7864 - bag_output_acc: 0.8089 - footwear_output_acc: 0.8102 - pose_output_acc: 0.8928 - emotion_output_acc: 0.7916\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 4.1114 - gender_output_loss: 0.2259 - image_quality_output_loss: 0.5831 - age_output_loss: 0.7301 - weight_output_loss: 0.5636 - bag_output_loss: 0.4789 - footwear_output_loss: 0.4670 - pose_output_loss: 0.2789 - emotion_output_loss: 0.5649 - gender_output_acc: 0.9039 - image_quality_output_acc: 0.7467 - age_output_acc: 0.7170 - weight_output_acc: 0.7864 - bag_output_acc: 0.8089 - footwear_output_acc: 0.8103 - pose_output_acc: 0.8928 - emotion_output_acc: 0.7915 - val_loss: 8.9030 - val_gender_output_loss: 0.4687 - val_image_quality_output_loss: 1.0842 - val_age_output_loss: 1.8741 - val_weight_output_loss: 1.1942 - val_bag_output_loss: 1.0326 - val_footwear_output_loss: 0.9144 - val_pose_output_loss: 0.6527 - val_emotion_output_loss: 1.1198 - val_gender_output_acc: 0.8246 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.3463 - val_weight_output_acc: 0.5801 - val_bag_output_acc: 0.6119 - val_footwear_output_acc: 0.6527 - val_pose_output_acc: 0.7757 - val_emotion_output_acc: 0.6699\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 7.24353\n",
            "Epoch 11/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.9287 - gender_output_loss: 0.2161 - image_quality_output_loss: 0.5526 - age_output_loss: 0.6875 - weight_output_loss: 0.5475 - bag_output_loss: 0.4681 - footwear_output_loss: 0.4532 - pose_output_loss: 0.2753 - emotion_output_loss: 0.5221 - gender_output_acc: 0.9083 - image_quality_output_acc: 0.7631 - age_output_acc: 0.7322 - weight_output_acc: 0.7890 - bag_output_acc: 0.8088 - footwear_output_acc: 0.8137 - pose_output_acc: 0.8955 - emotion_output_acc: 0.8133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bLearning rate:  0.001\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 3.9280 - gender_output_loss: 0.2165 - image_quality_output_loss: 0.5522 - age_output_loss: 0.6873 - weight_output_loss: 0.5471 - bag_output_loss: 0.4678 - footwear_output_loss: 0.4532 - pose_output_loss: 0.2760 - emotion_output_loss: 0.5217 - gender_output_acc: 0.9082 - image_quality_output_acc: 0.7635 - age_output_acc: 0.7322 - weight_output_acc: 0.7891 - bag_output_acc: 0.8089 - footwear_output_acc: 0.8137 - pose_output_acc: 0.8953 - emotion_output_acc: 0.8134 - val_loss: 9.1855 - val_gender_output_loss: 0.4531 - val_image_quality_output_loss: 1.1542 - val_age_output_loss: 1.9176 - val_weight_output_loss: 1.2406 - val_bag_output_loss: 1.0707 - val_footwear_output_loss: 0.9287 - val_pose_output_loss: 0.6854 - val_emotion_output_loss: 1.1600 - val_gender_output_acc: 0.8367 - val_image_quality_output_acc: 0.5333 - val_age_output_acc: 0.3594 - val_weight_output_acc: 0.5746 - val_bag_output_acc: 0.5943 - val_footwear_output_acc: 0.6376 - val_pose_output_acc: 0.7702 - val_emotion_output_acc: 0.6537\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 7.24353\n",
            "Epoch 12/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 3.8025 - gender_output_loss: 0.2065 - image_quality_output_loss: 0.5358 - age_output_loss: 0.6639 - weight_output_loss: 0.5232 - bag_output_loss: 0.4494 - footwear_output_loss: 0.4441 - pose_output_loss: 0.2771 - emotion_output_loss: 0.5034 - gender_output_acc: 0.9148 - image_quality_output_acc: 0.7692 - age_output_acc: 0.7418 - weight_output_acc: 0.7975 - bag_output_acc: 0.8160 - footwear_output_acc: 0.8152 - pose_output_acc: 0.8938 - emotion_output_acc: 0.8163 - val_loss: 9.4595 - val_gender_output_loss: 0.4671 - val_image_quality_output_loss: 1.1493 - val_age_output_loss: 1.9923 - val_weight_output_loss: 1.3014 - val_bag_output_loss: 1.1248 - val_footwear_output_loss: 0.9526 - val_pose_output_loss: 0.6916 - val_emotion_output_loss: 1.1828 - val_gender_output_acc: 0.8276 - val_image_quality_output_acc: 0.5338 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5464 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 7.24353\n",
            "Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.6014 - gender_output_loss: 0.2043 - image_quality_output_loss: 0.5114 - age_output_loss: 0.6221 - weight_output_loss: 0.4941 - bag_output_loss: 0.4255 - footwear_output_loss: 0.4232 - pose_output_loss: 0.2634 - emotion_output_loss: 0.4706 - gender_output_acc: 0.9145 - image_quality_output_acc: 0.7851 - age_output_acc: 0.7570 - weight_output_acc: 0.8121 - bag_output_acc: 0.8273 - footwear_output_acc: 0.8331 - pose_output_acc: 0.8975 - emotion_output_acc: 0.8351Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 3.6039 - gender_output_loss: 0.2046 - image_quality_output_loss: 0.5119 - age_output_loss: 0.6222 - weight_output_loss: 0.4936 - bag_output_loss: 0.4260 - footwear_output_loss: 0.4246 - pose_output_loss: 0.2632 - emotion_output_loss: 0.4710 - gender_output_acc: 0.9144 - image_quality_output_acc: 0.7848 - age_output_acc: 0.7569 - weight_output_acc: 0.8122 - bag_output_acc: 0.8270 - footwear_output_acc: 0.8326 - pose_output_acc: 0.8975 - emotion_output_acc: 0.8348 - val_loss: 9.8618 - val_gender_output_loss: 0.4902 - val_image_quality_output_loss: 1.2273 - val_age_output_loss: 2.0527 - val_weight_output_loss: 1.3313 - val_bag_output_loss: 1.1649 - val_footwear_output_loss: 0.9815 - val_pose_output_loss: 0.7211 - val_emotion_output_loss: 1.2771 - val_gender_output_acc: 0.8317 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3634 - val_weight_output_acc: 0.5726 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6361 - val_pose_output_acc: 0.7661 - val_emotion_output_acc: 0.6598\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 7.24353\n",
            "Epoch 14/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 3.3427 - gender_output_loss: 0.1916 - image_quality_output_loss: 0.4813 - age_output_loss: 0.5667 - weight_output_loss: 0.4604 - bag_output_loss: 0.3912 - footwear_output_loss: 0.3985 - pose_output_loss: 0.2480 - emotion_output_loss: 0.4349 - gender_output_acc: 0.9191 - image_quality_output_acc: 0.7987 - age_output_acc: 0.7846 - weight_output_acc: 0.8260 - bag_output_acc: 0.8470 - footwear_output_acc: 0.8382 - pose_output_acc: 0.9056 - emotion_output_acc: 0.8455\n",
            "Epoch 00013: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 3.3409 - gender_output_loss: 0.1913 - image_quality_output_loss: 0.4812 - age_output_loss: 0.5659 - weight_output_loss: 0.4604 - bag_output_loss: 0.3914 - footwear_output_loss: 0.3980 - pose_output_loss: 0.2482 - emotion_output_loss: 0.4348 - gender_output_acc: 0.9194 - image_quality_output_acc: 0.7986 - age_output_acc: 0.7849 - weight_output_acc: 0.8262 - bag_output_acc: 0.8469 - footwear_output_acc: 0.8385 - pose_output_acc: 0.9056 - emotion_output_acc: 0.8454 - val_loss: 10.0907 - val_gender_output_loss: 0.4756 - val_image_quality_output_loss: 1.2583 - val_age_output_loss: 2.1627 - val_weight_output_loss: 1.3520 - val_bag_output_loss: 1.1608 - val_footwear_output_loss: 1.0095 - val_pose_output_loss: 0.7249 - val_emotion_output_loss: 1.2981 - val_gender_output_acc: 0.8322 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.3599 - val_weight_output_acc: 0.5645 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7692 - val_emotion_output_acc: 0.6184\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 7.24353\n",
            "Epoch 15/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 3.2189 - gender_output_loss: 0.1880 - image_quality_output_loss: 0.4591 - age_output_loss: 0.5468 - weight_output_loss: 0.4352 - bag_output_loss: 0.3900 - footwear_output_loss: 0.3694 - pose_output_loss: 0.2425 - emotion_output_loss: 0.4238 - gender_output_acc: 0.9201 - image_quality_output_acc: 0.8095 - age_output_acc: 0.7924 - weight_output_acc: 0.8333 - bag_output_acc: 0.8470 - footwear_output_acc: 0.8556 - pose_output_acc: 0.9068 - emotion_output_acc: 0.8463 - val_loss: 10.4021 - val_gender_output_loss: 0.4984 - val_image_quality_output_loss: 1.3204 - val_age_output_loss: 2.2194 - val_weight_output_loss: 1.3898 - val_bag_output_loss: 1.1969 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 0.7456 - val_emotion_output_loss: 1.3276 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3498 - val_weight_output_acc: 0.5736 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7722 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 7.24353\n",
            "Epoch 16/100\n",
            "Learning rate:  0.001\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 3.1574 - gender_output_loss: 0.1885 - image_quality_output_loss: 0.4457 - age_output_loss: 0.5338 - weight_output_loss: 0.4239 - bag_output_loss: 0.3799 - footwear_output_loss: 0.3704 - pose_output_loss: 0.2495 - emotion_output_loss: 0.4057 - gender_output_acc: 0.9193 - image_quality_output_acc: 0.8155 - age_output_acc: 0.7970 - weight_output_acc: 0.8451 - bag_output_acc: 0.8450 - footwear_output_acc: 0.8515 - pose_output_acc: 0.9008 - emotion_output_acc: 0.8576 - val_loss: 10.7025 - val_gender_output_loss: 0.4944 - val_image_quality_output_loss: 1.3455 - val_age_output_loss: 2.3170 - val_weight_output_loss: 1.4365 - val_bag_output_loss: 1.2447 - val_footwear_output_loss: 1.0578 - val_pose_output_loss: 0.7487 - val_emotion_output_loss: 1.3629 - val_gender_output_acc: 0.8221 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5423 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.7676 - val_emotion_output_acc: 0.6290\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 7.24353\n",
            "Epoch 17/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.8742 - gender_output_loss: 0.1739 - image_quality_output_loss: 0.4041 - age_output_loss: 0.4791 - weight_output_loss: 0.3917 - bag_output_loss: 0.3521 - footwear_output_loss: 0.3430 - pose_output_loss: 0.2266 - emotion_output_loss: 0.3602 - gender_output_acc: 0.9277 - image_quality_output_acc: 0.8339 - age_output_acc: 0.8204 - weight_output_acc: 0.8538 - bag_output_acc: 0.8614 - footwear_output_acc: 0.8661 - pose_output_acc: 0.9159 - emotion_output_acc: 0.8734 - val_loss: 10.5069 - val_gender_output_loss: 0.5005 - val_image_quality_output_loss: 1.3252 - val_age_output_loss: 2.2300 - val_weight_output_loss: 1.4290 - val_bag_output_loss: 1.2163 - val_footwear_output_loss: 1.0529 - val_pose_output_loss: 0.7231 - val_emotion_output_loss: 1.3610 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.3443 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6346\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 7.24353\n",
            "Epoch 18/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.8433 - gender_output_loss: 0.1729 - image_quality_output_loss: 0.3973 - age_output_loss: 0.4785 - weight_output_loss: 0.3807 - bag_output_loss: 0.3517 - footwear_output_loss: 0.3390 - pose_output_loss: 0.2210 - emotion_output_loss: 0.3586 - gender_output_acc: 0.9302 - image_quality_output_acc: 0.8378 - age_output_acc: 0.8180 - weight_output_acc: 0.8594 - bag_output_acc: 0.8588 - footwear_output_acc: 0.8651 - pose_output_acc: 0.9168 - emotion_output_acc: 0.8774Learning rate:  0.0001\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 7.24353\n",
            "Epoch 18/100\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.8419 - gender_output_loss: 0.1731 - image_quality_output_loss: 0.3970 - age_output_loss: 0.4781 - weight_output_loss: 0.3801 - bag_output_loss: 0.3516 - footwear_output_loss: 0.3389 - pose_output_loss: 0.2207 - emotion_output_loss: 0.3588 - gender_output_acc: 0.9301 - image_quality_output_acc: 0.8380 - age_output_acc: 0.8181 - weight_output_acc: 0.8596 - bag_output_acc: 0.8588 - footwear_output_acc: 0.8652 - pose_output_acc: 0.9170 - emotion_output_acc: 0.8774 - val_loss: 10.6165 - val_gender_output_loss: 0.5003 - val_image_quality_output_loss: 1.3491 - val_age_output_loss: 2.2507 - val_weight_output_loss: 1.4431 - val_bag_output_loss: 1.2356 - val_footwear_output_loss: 1.0613 - val_pose_output_loss: 0.7278 - val_emotion_output_loss: 1.3733 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.3458 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6376\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.24353\n",
            "Epoch 19/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.7424 - gender_output_loss: 0.1698 - image_quality_output_loss: 0.3963 - age_output_loss: 0.4574 - weight_output_loss: 0.3711 - bag_output_loss: 0.3316 - footwear_output_loss: 0.3215 - pose_output_loss: 0.2126 - emotion_output_loss: 0.3448 - gender_output_acc: 0.9324 - image_quality_output_acc: 0.8391 - age_output_acc: 0.8318 - weight_output_acc: 0.8641 - bag_output_acc: 0.8705 - footwear_output_acc: 0.8737 - pose_output_acc: 0.9190 - emotion_output_acc: 0.8796 - val_loss: 10.5645 - val_gender_output_loss: 0.4916 - val_image_quality_output_loss: 1.3424 - val_age_output_loss: 2.2467 - val_weight_output_loss: 1.4382 - val_bag_output_loss: 1.2344 - val_footwear_output_loss: 1.0482 - val_pose_output_loss: 0.7246 - val_emotion_output_loss: 1.3644 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6341\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.24353\n",
            "Epoch 20/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.6270 - gender_output_loss: 0.1654 - image_quality_output_loss: 0.3810 - age_output_loss: 0.4295 - weight_output_loss: 0.3545 - bag_output_loss: 0.3125 - footwear_output_loss: 0.3179 - pose_output_loss: 0.2116 - emotion_output_loss: 0.3257 - gender_output_acc: 0.9307 - image_quality_output_acc: 0.8437 - age_output_acc: 0.8421 - weight_output_acc: 0.8727 - bag_output_acc: 0.8749 - footwear_output_acc: 0.8721 - pose_output_acc: 0.9169 - emotion_output_acc: 0.8891 - val_loss: 10.6202 - val_gender_output_loss: 0.4922 - val_image_quality_output_loss: 1.3527 - val_age_output_loss: 2.2496 - val_weight_output_loss: 1.4494 - val_bag_output_loss: 1.2427 - val_footwear_output_loss: 1.0586 - val_pose_output_loss: 0.7273 - val_emotion_output_loss: 1.3729 - val_gender_output_acc: 0.8241 - val_image_quality_output_acc: 0.5272 - val_age_output_acc: 0.3523 - val_weight_output_acc: 0.5655 - val_bag_output_acc: 0.5922 - val_footwear_output_acc: 0.6376 - val_pose_output_acc: 0.7757 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 7.24353\n",
            "Epoch 21/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.6427 - gender_output_loss: 0.1702 - image_quality_output_loss: 0.3749 - age_output_loss: 0.4294 - weight_output_loss: 0.3562 - bag_output_loss: 0.3144 - footwear_output_loss: 0.3195 - pose_output_loss: 0.2099 - emotion_output_loss: 0.3394 - gender_output_acc: 0.9278 - image_quality_output_acc: 0.8462 - age_output_acc: 0.8385 - weight_output_acc: 0.8657 - bag_output_acc: 0.8755 - footwear_output_acc: 0.8740 - pose_output_acc: 0.9205 - emotion_output_acc: 0.8808 - val_loss: 10.7259 - val_gender_output_loss: 0.4925 - val_image_quality_output_loss: 1.3673 - val_age_output_loss: 2.2829 - val_weight_output_loss: 1.4513 - val_bag_output_loss: 1.2464 - val_footwear_output_loss: 1.0771 - val_pose_output_loss: 0.7323 - val_emotion_output_loss: 1.3911 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3488 - val_weight_output_acc: 0.5570 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7807 - val_emotion_output_acc: 0.6376\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 7.24353\n",
            "Epoch 22/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.4915 - gender_output_loss: 0.1566 - image_quality_output_loss: 0.3569 - age_output_loss: 0.3993 - weight_output_loss: 0.3390 - bag_output_loss: 0.2987 - footwear_output_loss: 0.3005 - pose_output_loss: 0.2010 - emotion_output_loss: 0.3197 - gender_output_acc: 0.9356 - image_quality_output_acc: 0.8579 - age_output_acc: 0.8519 - weight_output_acc: 0.8768 - bag_output_acc: 0.8828 - footwear_output_acc: 0.8813 - pose_output_acc: 0.9224 - emotion_output_acc: 0.8920 - 52s 143ms/step - loss: 2.6427 - gender_output_loss: 0.1702 - image_quality_output_loss: 0.3749 - age_output_loss: 0.4294 - weight_output_loss: 0.3562 - bag_output_loss: 0.3144 - footwear_output_loss: 0.3195 - pose_output_loss: 0.2099 - emotion_output_loss: 0.3394 - gender_output_acc: 0.9278 - image_quality_output_acc: 0.8462 - age_output_acc: 0.8385 - weight_output_acc: 0.8657 - bag_output_acc: 0.8755 - footwear_output_acc: 0.8740 - pose_output_acc: 0.9205 - emotion_output_acc: 0.8808 - val_loss: 10.7259 - val_gender_output_loss: 0.4925 - val_image_quality_output_loss: 1.3673 - val_age_output_loss: 2.2829 - val_weight_output_loss: 1.4513 - val_bag_output_loss: 1.2464 - val_footwear_output_loss: 1.0771 - val_pose_output_loss: 0.7323 - val_emotion_output_loss: 1.3911 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3488 - val_weight_output_acc: 0.5570 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7807 - val_emotion_output_acc: 0.6376\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.4928 - gender_output_loss: 0.1569 - image_quality_output_loss: 0.3570 - age_output_loss: 0.3993 - weight_output_loss: 0.3389 - bag_output_loss: 0.2994 - footwear_output_loss: 0.3005 - pose_output_loss: 0.2013 - emotion_output_loss: 0.3197 - gender_output_acc: 0.9353 - image_quality_output_acc: 0.8578 - age_output_acc: 0.8520 - weight_output_acc: 0.8769 - bag_output_acc: 0.8827 - footwear_output_acc: 0.8813 - pose_output_acc: 0.9223 - emotion_output_acc: 0.8918 - val_loss: 10.7271 - val_gender_output_loss: 0.4953 - val_image_quality_output_loss: 1.3727 - val_age_output_loss: 2.2766 - val_weight_output_loss: 1.4563 - val_bag_output_loss: 1.2504 - val_footwear_output_loss: 1.0661 - val_pose_output_loss: 0.7360 - val_emotion_output_loss: 1.3907 - val_gender_output_acc: 0.8226 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3528 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6336\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 7.24353\n",
            "Epoch 23/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.4271 - gender_output_loss: 0.1534 - image_quality_output_loss: 0.3514 - age_output_loss: 0.3938 - weight_output_loss: 0.3264 - bag_output_loss: 0.2938 - footwear_output_loss: 0.2946 - pose_output_loss: 0.1886 - emotion_output_loss: 0.3069 - gender_output_acc: 0.9364 - image_quality_output_acc: 0.8598 - age_output_acc: 0.8541 - weight_output_acc: 0.8833 - bag_output_acc: 0.8862 - footwear_output_acc: 0.8847 - pose_output_acc: 0.9287 - emotion_output_acc: 0.8967\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 23/100\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.4267 - gender_output_loss: 0.1534 - image_quality_output_loss: 0.3512 - age_output_loss: 0.3939 - weight_output_loss: 0.3260 - bag_output_loss: 0.2943 - footwear_output_loss: 0.2947 - pose_output_loss: 0.1884 - emotion_output_loss: 0.3065 - gender_output_acc: 0.9363 - image_quality_output_acc: 0.8597 - age_output_acc: 0.8540 - weight_output_acc: 0.8833 - bag_output_acc: 0.8861 - footwear_output_acc: 0.8846 - pose_output_acc: 0.9288 - emotion_output_acc: 0.8969 - val_loss: 10.7318 - val_gender_output_loss: 0.4966 - val_image_quality_output_loss: 1.3780 - val_age_output_loss: 2.2772 - val_weight_output_loss: 1.4540 - val_bag_output_loss: 1.2515 - val_footwear_output_loss: 1.0693 - val_pose_output_loss: 0.7291 - val_emotion_output_loss: 1.3929 - val_gender_output_acc: 0.8241 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.3503 - val_weight_output_acc: 0.5640 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.7818 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 7.24353\n",
            "Epoch 24/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3836 - gender_output_loss: 0.1485 - image_quality_output_loss: 0.3463 - age_output_loss: 0.3910 - weight_output_loss: 0.3222 - bag_output_loss: 0.2884 - footwear_output_loss: 0.2854 - pose_output_loss: 0.1868 - emotion_output_loss: 0.2977 - gender_output_acc: 0.9404 - image_quality_output_acc: 0.8647 - age_output_acc: 0.8598 - weight_output_acc: 0.8852 - bag_output_acc: 0.8917 - footwear_output_acc: 0.8892 - pose_output_acc: 0.9290 - emotion_output_acc: 0.8984\n",
            "Epoch 00023: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3828 - gender_output_loss: 0.1483 - image_quality_output_loss: 0.3462 - age_output_loss: 0.3911 - weight_output_loss: 0.3222 - bag_output_loss: 0.2883 - footwear_output_loss: 0.2854 - pose_output_loss: 0.1865 - emotion_output_loss: 0.2974 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.8648 - age_output_acc: 0.8596 - weight_output_acc: 0.8852 - bag_output_acc: 0.8918 - footwear_output_acc: 0.8893 - pose_output_acc: 0.9291 - emotion_output_acc: 0.8984 - val_loss: 10.7736 - val_gender_output_loss: 0.4992 - val_image_quality_output_loss: 1.3780 - val_age_output_loss: 2.2824 - val_weight_output_loss: 1.4618 - val_bag_output_loss: 1.2578 - val_footwear_output_loss: 1.0772 - val_pose_output_loss: 0.7303 - val_emotion_output_loss: 1.4023 - val_gender_output_acc: 0.8231 - val_image_quality_output_acc: 0.5272 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6406\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.24353\n",
            "Epoch 25/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0001.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.4453 - gender_output_loss: 0.1529 - image_quality_output_loss: 0.3437 - age_output_loss: 0.4086 - weight_output_loss: 0.3310 - bag_output_loss: 0.2912 - footwear_output_loss: 0.2893 - pose_output_loss: 0.1943 - emotion_output_loss: 0.3116 - gender_output_acc: 0.9368 - image_quality_output_acc: 0.8646 - age_output_acc: 0.8457 - weight_output_acc: 0.8790 - bag_output_acc: 0.8838 - footwear_output_acc: 0.8870 - pose_output_acc: 0.9241 - emotion_output_acc: 0.8970 - val_loss: 10.9810 - val_gender_output_loss: 0.5054 - val_image_quality_output_loss: 1.4060 - val_age_output_loss: 2.3408 - val_weight_output_loss: 1.4843 - val_bag_output_loss: 1.2824 - val_footwear_output_loss: 1.1032 - val_pose_output_loss: 0.7429 - val_emotion_output_loss: 1.4139 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5691 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6406 - val_pose_output_acc: 0.7807 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 7.24353\n",
            "Epoch 26/100\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0001.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.4430 - gender_output_loss: 0.1528 - image_quality_output_loss: 0.3452 - age_output_loss: 0.3993 - weight_output_loss: 0.3298 - bag_output_loss: 0.3028 - footwear_output_loss: 0.2901 - pose_output_loss: 0.1960 - emotion_output_loss: 0.3071 - gender_output_acc: 0.9361 - image_quality_output_acc: 0.8626 - age_output_acc: 0.8537 - weight_output_acc: 0.8787 - bag_output_acc: 0.8820 - footwear_output_acc: 0.8888 - pose_output_acc: 0.9258 - emotion_output_acc: 0.8955 0.0001\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.4418 - gender_output_loss: 0.1528 - image_quality_output_loss: 0.3452 - age_output_loss: 0.3992 - weight_output_loss: 0.3293 - bag_output_loss: 0.3025 - footwear_output_loss: 0.2897 - pose_output_loss: 0.1961 - emotion_output_loss: 0.3073 - gender_output_acc: 0.9362 - image_quality_output_acc: 0.8628 - age_output_acc: 0.8536 - weight_output_acc: 0.8789 - bag_output_acc: 0.8820 - footwear_output_acc: 0.8891 - pose_output_acc: 0.9258 - emotion_output_acc: 0.8956 - val_loss: 10.9677 - val_gender_output_loss: 0.5047 - val_image_quality_output_loss: 1.4223 - val_age_output_loss: 2.3271 - val_weight_output_loss: 1.4941 - val_bag_output_loss: 1.2736 - val_footwear_output_loss: 1.0929 - val_pose_output_loss: 0.7403 - val_emotion_output_loss: 1.4147 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5161 - val_age_output_acc: 0.3523 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6376\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 7.24353\n",
            "Epoch 27/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 1e-05.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.4191 - gender_output_loss: 0.1591 - image_quality_output_loss: 0.3420 - age_output_loss: 0.3911 - weight_output_loss: 0.3274 - bag_output_loss: 0.2880 - footwear_output_loss: 0.2874 - pose_output_loss: 0.1955 - emotion_output_loss: 0.3114 - gender_output_acc: 0.9350 - image_quality_output_acc: 0.8616 - age_output_acc: 0.8537 - weight_output_acc: 0.8824 - bag_output_acc: 0.8895 - footwear_output_acc: 0.8886 - pose_output_acc: 0.9253 - emotion_output_acc: 0.8936\n",
            "Epoch 00026: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 146ms/step - loss: 2.4183 - gender_output_loss: 0.1590 - image_quality_output_loss: 0.3417 - age_output_loss: 0.3908 - weight_output_loss: 0.3274 - bag_output_loss: 0.2880 - footwear_output_loss: 0.2873 - pose_output_loss: 0.1952 - emotion_output_loss: 0.3117 - gender_output_acc: 0.9349 - image_quality_output_acc: 0.8618 - age_output_acc: 0.8537 - weight_output_acc: 0.8823 - bag_output_acc: 0.8893 - footwear_output_acc: 0.8886 - pose_output_acc: 0.9255 - emotion_output_acc: 0.8934 - val_loss: 11.0834 - val_gender_output_loss: 0.5107 - val_image_quality_output_loss: 1.4378 - val_age_output_loss: 2.3555 - val_weight_output_loss: 1.5059 - val_bag_output_loss: 1.2917 - val_footwear_output_loss: 1.1108 - val_pose_output_loss: 0.7429 - val_emotion_output_loss: 1.4215 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5534 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6321\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 7.24353\n",
            "Epoch 28/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.4992 - gender_output_loss: 0.1632 - image_quality_output_loss: 0.3574 - age_output_loss: 0.4083 - weight_output_loss: 0.3275 - bag_output_loss: 0.2999 - footwear_output_loss: 0.3043 - pose_output_loss: 0.1999 - emotion_output_loss: 0.3161 - gender_output_acc: 0.9318 - image_quality_output_acc: 0.8532 - age_output_acc: 0.8441 - weight_output_acc: 0.8806 - bag_output_acc: 0.8831 - footwear_output_acc: 0.8796 - pose_output_acc: 0.9240 - emotion_output_acc: 0.8931 - val_loss: 11.0442 - val_gender_output_loss: 0.5087 - val_image_quality_output_loss: 1.4263 - val_age_output_loss: 2.3498 - val_weight_output_loss: 1.5012 - val_bag_output_loss: 1.2891 - val_footwear_output_loss: 1.1032 - val_pose_output_loss: 0.7412 - val_emotion_output_loss: 1.4198 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.3518 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7767 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 7.24353\n",
            "Epoch 29/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3868 - gender_output_loss: 0.1535 - image_quality_output_loss: 0.3397 - age_output_loss: 0.3831 - weight_output_loss: 0.3225 - bag_output_loss: 0.2906 - footwear_output_loss: 0.2841 - pose_output_loss: 0.1917 - emotion_output_loss: 0.3068 - gender_output_acc: 0.9375 - image_quality_output_acc: 0.8674 - age_output_acc: 0.8549 - weight_output_acc: 0.8829 - bag_output_acc: 0.8852 - footwear_output_acc: 0.8899 - pose_output_acc: 0.9262 - emotion_output_acc: 0.8948 - val_loss: 10.9983 - val_gender_output_loss: 0.5050 - val_image_quality_output_loss: 1.4235 - val_age_output_loss: 2.3335 - val_weight_output_loss: 1.4912 - val_bag_output_loss: 1.2845 - val_footwear_output_loss: 1.0996 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 1.4200 - val_gender_output_acc: 0.8276 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3508 - val_weight_output_acc: 0.5565 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.7767 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 7.24353\n",
            "Epoch 30/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3536 - gender_output_loss: 0.1507 - image_quality_output_loss: 0.3366 - age_output_loss: 0.3800 - weight_output_loss: 0.3109 - bag_output_loss: 0.2823 - footwear_output_loss: 0.2810 - pose_output_loss: 0.2003 - emotion_output_loss: 0.2978 - gender_output_acc: 0.9385 - image_quality_output_acc: 0.8659 - age_output_acc: 0.8623 - weight_output_acc: 0.8896 - bag_output_acc: 0.8914 - footwear_output_acc: 0.8901 - pose_output_acc: 0.9266 - emotion_output_acc: 0.8984 - val_loss: 11.0603 - val_gender_output_loss: 0.5087 - val_image_quality_output_loss: 1.4315 - val_age_output_loss: 2.3527 - val_weight_output_loss: 1.4990 - val_bag_output_loss: 1.2927 - val_footwear_output_loss: 1.1050 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 1.4224 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5554 - val_bag_output_acc: 0.5842 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7767 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 7.24353\n",
            "Epoch 31/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3315 - gender_output_loss: 0.1449 - image_quality_output_loss: 0.3302 - age_output_loss: 0.3839 - weight_output_loss: 0.3135 - bag_output_loss: 0.2842 - footwear_output_loss: 0.2782 - pose_output_loss: 0.1832 - emotion_output_loss: 0.2983 - gender_output_acc: 0.9434 - image_quality_output_acc: 0.8700 - age_output_acc: 0.8586 - weight_output_acc: 0.8866 - bag_output_acc: 0.8899 - footwear_output_acc: 0.8936 - pose_output_acc: 0.9312 - emotion_output_acc: 0.9010 - val_loss: 11.0498 - val_gender_output_loss: 0.5093 - val_image_quality_output_loss: 1.4294 - val_age_output_loss: 2.3473 - val_weight_output_loss: 1.4980 - val_bag_output_loss: 1.2937 - val_footwear_output_loss: 1.1031 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 1.4240 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5186 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5625 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6396\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 7.24353\n",
            "Epoch 32/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3315 - gender_output_loss: 0.1449 - image_quality_output_loss: 0.3302 - age_output_loss: 0.3839 - weight_output_loss: 0.3135 - bag_output_loss: 0.2842 - footwear_output_loss: 0.2782 - pose_output_loss: 0.1832 - emotion_output_loss: 0.2983 - gender_output_acc: 0.9434 - image_quality_output_acc: 0.8700 - age_output_acc: 0.8586 - weight_output_acc: 0.8866 - bag_output_acc: 0.8899 - footwear_output_acc: 0.8936 - pose_output_acc: 0.9312 - emotion_output_acc: 0.9010 - val_loss: 11.0498 - val_gender_output_loss: 0.5093 - val_image_quality_output_loss: 1.4294 - val_age_output_loss: 2.3473 - val_weight_output_loss: 1.4980 - val_bag_output_loss: 1.2937 - val_footwear_output_loss: 1.1031 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 1.4240 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5186 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5625 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6396\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.4056 - gender_output_loss: 0.1510 - image_quality_output_loss: 0.3402 - age_output_loss: 0.3963 - weight_output_loss: 0.3310 - bag_output_loss: 0.2909 - footwear_output_loss: 0.2846 - pose_output_loss: 0.1960 - emotion_output_loss: 0.2966 - gender_output_acc: 0.9373 - image_quality_output_acc: 0.8652 - age_output_acc: 0.8538 - weight_output_acc: 0.8784 - bag_output_acc: 0.8865 - footwear_output_acc: 0.8897 - pose_output_acc: 0.9244 - emotion_output_acc: 0.9011 - val_loss: 11.0789 - val_gender_output_loss: 0.5079 - val_image_quality_output_loss: 1.4349 - val_age_output_loss: 2.3558 - val_weight_output_loss: 1.5045 - val_bag_output_loss: 1.2933 - val_footwear_output_loss: 1.1050 - val_pose_output_loss: 0.7465 - val_emotion_output_loss: 1.4243 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3523 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 7.24353\n",
            "Epoch 33/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3310 - gender_output_loss: 0.1468 - image_quality_output_loss: 0.3321 - age_output_loss: 0.3793 - weight_output_loss: 0.3093 - bag_output_loss: 0.2801 - footwear_output_loss: 0.2891 - pose_output_loss: 0.1868 - emotion_output_loss: 0.2936 - gender_output_acc: 0.9392 - image_quality_output_acc: 0.8678 - age_output_acc: 0.8576 - weight_output_acc: 0.8867 - bag_output_acc: 0.8931 - footwear_output_acc: 0.8892 - pose_output_acc: 0.9291 - emotion_output_acc: 0.9021 - val_loss: 11.0155 - val_gender_output_loss: 0.5055 - val_image_quality_output_loss: 1.4256 - val_age_output_loss: 2.3390 - val_weight_output_loss: 1.4984 - val_bag_output_loss: 1.2843 - val_footwear_output_loss: 1.0993 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 1.4209 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5227 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7802 - val_emotion_output_acc: 0.6336\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 7.24353\n",
            "Epoch 34/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 1e-05.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.4012 - gender_output_loss: 0.1532 - image_quality_output_loss: 0.3372 - age_output_loss: 0.3906 - weight_output_loss: 0.3253 - bag_output_loss: 0.2897 - footwear_output_loss: 0.2878 - pose_output_loss: 0.1949 - emotion_output_loss: 0.3053 - gender_output_acc: 0.9378 - image_quality_output_acc: 0.8666 - age_output_acc: 0.8565 - weight_output_acc: 0.8786 - bag_output_acc: 0.8861 - footwear_output_acc: 0.8885 - pose_output_acc: 0.9257 - emotion_output_acc: 0.8991\n",
            "Epoch 34/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.4009 - gender_output_loss: 0.1531 - image_quality_output_loss: 0.3372 - age_output_loss: 0.3906 - weight_output_loss: 0.3254 - bag_output_loss: 0.2898 - footwear_output_loss: 0.2877 - pose_output_loss: 0.1946 - emotion_output_loss: 0.3054 - gender_output_acc: 0.9378 - image_quality_output_acc: 0.8666 - age_output_acc: 0.8562 - weight_output_acc: 0.8785 - bag_output_acc: 0.8859 - footwear_output_acc: 0.8886 - pose_output_acc: 0.9259 - emotion_output_acc: 0.8991 - val_loss: 11.0692 - val_gender_output_loss: 0.5084 - val_image_quality_output_loss: 1.4328 - val_age_output_loss: 2.3516 - val_weight_output_loss: 1.5034 - val_bag_output_loss: 1.2921 - val_footwear_output_loss: 1.1074 - val_pose_output_loss: 0.7430 - val_emotion_output_loss: 1.4250 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5570 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 7.24353\n",
            "Epoch 35/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.4058 - gender_output_loss: 0.1517 - image_quality_output_loss: 0.3396 - age_output_loss: 0.3955 - weight_output_loss: 0.3258 - bag_output_loss: 0.2959 - footwear_output_loss: 0.2771 - pose_output_loss: 0.1963 - emotion_output_loss: 0.3051 - gender_output_acc: 0.9370 - image_quality_output_acc: 0.8622 - age_output_acc: 0.8529 - weight_output_acc: 0.8840 - bag_output_acc: 0.8845 - footwear_output_acc: 0.8959 - pose_output_acc: 0.9249 - emotion_output_acc: 0.8957 - val_loss: 11.0929 - val_gender_output_loss: 0.5081 - val_image_quality_output_loss: 1.4367 - val_age_output_loss: 2.3538 - val_weight_output_loss: 1.5092 - val_bag_output_loss: 1.2932 - val_footwear_output_loss: 1.1067 - val_pose_output_loss: 0.7442 - val_emotion_output_loss: 1.4349 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3513 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6411\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 7.24353\n",
            "Epoch 36/100\n",
            "Learning rate:  1e-05\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 1e-05.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3916 - gender_output_loss: 0.1422 - image_quality_output_loss: 0.3438 - age_output_loss: 0.3997 - weight_output_loss: 0.3196 - bag_output_loss: 0.2880 - footwear_output_loss: 0.2882 - pose_output_loss: 0.1888 - emotion_output_loss: 0.3014 - gender_output_acc: 0.9421 - image_quality_output_acc: 0.8580 - age_output_acc: 0.8497 - weight_output_acc: 0.8808 - bag_output_acc: 0.8857 - footwear_output_acc: 0.8889 - pose_output_acc: 0.9312 - emotion_output_acc: 0.8987 - val_loss: 11.0419 - val_gender_output_loss: 0.5060 - val_image_quality_output_loss: 1.4304 - val_age_output_loss: 2.3460 - val_weight_output_loss: 1.5023 - val_bag_output_loss: 1.2857 - val_footwear_output_loss: 1.1016 - val_pose_output_loss: 0.7417 - val_emotion_output_loss: 1.4245 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5565 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7818 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 7.24353\n",
            "Epoch 37/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3716 - gender_output_loss: 0.1467 - image_quality_output_loss: 0.3354 - age_output_loss: 0.3833 - weight_output_loss: 0.3241 - bag_output_loss: 0.2912 - footwear_output_loss: 0.2862 - pose_output_loss: 0.1925 - emotion_output_loss: 0.2971 - gender_output_acc: 0.9411 - image_quality_output_acc: 0.8634 - age_output_acc: 0.8562 - weight_output_acc: 0.8816 - bag_output_acc: 0.8879 - footwear_output_acc: 0.8869 - pose_output_acc: 0.9258 - emotion_output_acc: 0.9010 - val_loss: 11.0661 - val_gender_output_loss: 0.5073 - val_image_quality_output_loss: 1.4314 - val_age_output_loss: 2.3516 - val_weight_output_loss: 1.5039 - val_bag_output_loss: 1.2906 - val_footwear_output_loss: 1.1076 - val_pose_output_loss: 0.7434 - val_emotion_output_loss: 1.4247 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3523 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 7.24353\n",
            "Epoch 38/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 1e-06.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3403 - gender_output_loss: 0.1480 - image_quality_output_loss: 0.3303 - age_output_loss: 0.3839 - weight_output_loss: 0.3165 - bag_output_loss: 0.2818 - footwear_output_loss: 0.2832 - pose_output_loss: 0.1833 - emotion_output_loss: 0.2979 - gender_output_acc: 0.9399 - image_quality_output_acc: 0.8718 - age_output_acc: 0.8611 - weight_output_acc: 0.8855 - bag_output_acc: 0.8935 - footwear_output_acc: 0.8894 - pose_output_acc: 0.9310 - emotion_output_acc: 0.8995\n",
            "Epoch 00037: val_loss did not improve from 7.24353\n",
            "Epoch 38/100\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3412 - gender_output_loss: 0.1480 - image_quality_output_loss: 0.3307 - age_output_loss: 0.3838 - weight_output_loss: 0.3165 - bag_output_loss: 0.2818 - footwear_output_loss: 0.2834 - pose_output_loss: 0.1839 - emotion_output_loss: 0.2979 - gender_output_acc: 0.9398 - image_quality_output_acc: 0.8715 - age_output_acc: 0.8610 - weight_output_acc: 0.8856 - bag_output_acc: 0.8935 - footwear_output_acc: 0.8892 - pose_output_acc: 0.9307 - emotion_output_acc: 0.8997 - val_loss: 11.0985 - val_gender_output_loss: 0.5113 - val_image_quality_output_loss: 1.4345 - val_age_output_loss: 2.3587 - val_weight_output_loss: 1.5068 - val_bag_output_loss: 1.2959 - val_footwear_output_loss: 1.1053 - val_pose_output_loss: 0.7455 - val_emotion_output_loss: 1.4329 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 7.24353\n",
            "Epoch 39/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3333 - gender_output_loss: 0.1500 - image_quality_output_loss: 0.3352 - age_output_loss: 0.3818 - weight_output_loss: 0.3120 - bag_output_loss: 0.2701 - footwear_output_loss: 0.2897 - pose_output_loss: 0.1819 - emotion_output_loss: 0.2980 - gender_output_acc: 0.9409 - image_quality_output_acc: 0.8693 - age_output_acc: 0.8572 - weight_output_acc: 0.8900 - bag_output_acc: 0.8966 - footwear_output_acc: 0.8885 - pose_output_acc: 0.9327 - emotion_output_acc: 0.8987 - val_loss: 11.0044 - val_gender_output_loss: 0.5062 - val_image_quality_output_loss: 1.4210 - val_age_output_loss: 2.3400 - val_weight_output_loss: 1.4926 - val_bag_output_loss: 1.2850 - val_footwear_output_loss: 1.0997 - val_pose_output_loss: 0.7368 - val_emotion_output_loss: 1.4209 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 7.24353\n",
            "Epoch 40/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3132 - gender_output_loss: 0.1469 - image_quality_output_loss: 0.3339 - age_output_loss: 0.3732 - weight_output_loss: 0.3119 - bag_output_loss: 0.2847 - footwear_output_loss: 0.2829 - pose_output_loss: 0.1839 - emotion_output_loss: 0.2839 - gender_output_acc: 0.9392 - image_quality_output_acc: 0.8678 - age_output_acc: 0.8603 - weight_output_acc: 0.8832 - bag_output_acc: 0.8858 - footwear_output_acc: 0.8910 - pose_output_acc: 0.9307 - emotion_output_acc: 0.9042 - val_loss: 11.0697 - val_gender_output_loss: 0.5083 - val_image_quality_output_loss: 1.4316 - val_age_output_loss: 2.3568 - val_weight_output_loss: 1.4996 - val_bag_output_loss: 1.2925 - val_footwear_output_loss: 1.1031 - val_pose_output_loss: 0.7414 - val_emotion_output_loss: 1.4293 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3594 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6462 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 7.24353\n",
            "Epoch 41/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3935 - gender_output_loss: 0.1546 - image_quality_output_loss: 0.3416 - age_output_loss: 0.3905 - weight_output_loss: 0.3213 - bag_output_loss: 0.2919 - footwear_output_loss: 0.2810 - pose_output_loss: 0.1929 - emotion_output_loss: 0.3027 - gender_output_acc: 0.9379 - image_quality_output_acc: 0.8612 - age_output_acc: 0.8559 - weight_output_acc: 0.8838 - bag_output_acc: 0.8841 - footwear_output_acc: 0.8906 - pose_output_acc: 0.9264 - emotion_output_acc: 0.9000 - val_loss: 11.1066 - val_gender_output_loss: 0.5101 - val_image_quality_output_loss: 1.4378 - val_age_output_loss: 2.3603 - val_weight_output_loss: 1.5073 - val_bag_output_loss: 1.2953 - val_footwear_output_loss: 1.1086 - val_pose_output_loss: 0.7460 - val_emotion_output_loss: 1.4331 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3528 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6462 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 7.24353\n",
            "Epoch 42/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 1e-06.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3347 - gender_output_loss: 0.1496 - image_quality_output_loss: 0.3330 - age_output_loss: 0.3768 - weight_output_loss: 0.3215 - bag_output_loss: 0.2889 - footwear_output_loss: 0.2776 - pose_output_loss: 0.1786 - emotion_output_loss: 0.2956 - gender_output_acc: 0.9385 - image_quality_output_acc: 0.8683 - age_output_acc: 0.8604 - weight_output_acc: 0.8851 - bag_output_acc: 0.8874 - footwear_output_acc: 0.8915 - pose_output_acc: 0.9314 - emotion_output_acc: 0.9004Learning rate:  1e-06\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3349 - gender_output_loss: 0.1494 - image_quality_output_loss: 0.3329 - age_output_loss: 0.3773 - weight_output_loss: 0.3214 - bag_output_loss: 0.2894 - footwear_output_loss: 0.2773 - pose_output_loss: 0.1787 - emotion_output_loss: 0.2954 - gender_output_acc: 0.9387 - image_quality_output_acc: 0.8683 - age_output_acc: 0.8601 - weight_output_acc: 0.8851 - bag_output_acc: 0.8873 - footwear_output_acc: 0.8916 - pose_output_acc: 0.9313 - emotion_output_acc: 0.9005 - val_loss: 11.0719 - val_gender_output_loss: 0.5077 - val_image_quality_output_loss: 1.4291 - val_age_output_loss: 2.3538 - val_weight_output_loss: 1.5058 - val_bag_output_loss: 1.2898 - val_footwear_output_loss: 1.1067 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 1.4303 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3548 - val_weight_output_acc: 0.5595 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6376\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 7.24353\n",
            "Epoch 43/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3289 - gender_output_loss: 0.1482 - image_quality_output_loss: 0.3394 - age_output_loss: 0.3827 - weight_output_loss: 0.3136 - bag_output_loss: 0.2812 - footwear_output_loss: 0.2720 - pose_output_loss: 0.1839 - emotion_output_loss: 0.2931 - gender_output_acc: 0.9394 - image_quality_output_acc: 0.8649 - age_output_acc: 0.8576 - weight_output_acc: 0.8842 - bag_output_acc: 0.8936 - footwear_output_acc: 0.8968 - pose_output_acc: 0.9307 - emotion_output_acc: 0.9034 - val_loss: 11.0029 - val_gender_output_loss: 0.5039 - val_image_quality_output_loss: 1.4277 - val_age_output_loss: 2.3307 - val_weight_output_loss: 1.4973 - val_bag_output_loss: 1.2862 - val_footwear_output_loss: 1.0971 - val_pose_output_loss: 0.7381 - val_emotion_output_loss: 1.4227 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3594 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 7.24353\n",
            "Epoch 44/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3407 - gender_output_loss: 0.1553 - image_quality_output_loss: 0.3297 - age_output_loss: 0.3863 - weight_output_loss: 0.3096 - bag_output_loss: 0.2732 - footwear_output_loss: 0.2844 - pose_output_loss: 0.1911 - emotion_output_loss: 0.2951 - gender_output_acc: 0.9356 - image_quality_output_acc: 0.8696 - age_output_acc: 0.8541 - weight_output_acc: 0.8900 - bag_output_acc: 0.8954 - footwear_output_acc: 0.8873 - pose_output_acc: 0.9269 - emotion_output_acc: 0.8989 - val_loss: 11.0050 - val_gender_output_loss: 0.5048 - val_image_quality_output_loss: 1.4274 - val_age_output_loss: 2.3326 - val_weight_output_loss: 1.4981 - val_bag_output_loss: 1.2825 - val_footwear_output_loss: 1.1016 - val_pose_output_loss: 0.7403 - val_emotion_output_loss: 1.4179 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 7.24353\n",
            "Epoch 45/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3279 - gender_output_loss: 0.1462 - image_quality_output_loss: 0.3297 - age_output_loss: 0.3817 - weight_output_loss: 0.3138 - bag_output_loss: 0.2856 - footwear_output_loss: 0.2747 - pose_output_loss: 0.1842 - emotion_output_loss: 0.2974 - gender_output_acc: 0.9389 - image_quality_output_acc: 0.8701 - age_output_acc: 0.8576 - weight_output_acc: 0.8884 - bag_output_acc: 0.8921 - footwear_output_acc: 0.8907 - pose_output_acc: 0.9326 - emotion_output_acc: 0.8975 - val_loss: 11.0583 - val_gender_output_loss: 0.5072 - val_image_quality_output_loss: 1.4365 - val_age_output_loss: 2.3481 - val_weight_output_loss: 1.5030 - val_bag_output_loss: 1.2879 - val_footwear_output_loss: 1.1048 - val_pose_output_loss: 0.7419 - val_emotion_output_loss: 1.4245 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3533 - val_weight_output_acc: 0.5575 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6356\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 7.24353\n",
            "Epoch 46/100\n",
            "Learning rate:  1e-06\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 1e-06.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3208 - gender_output_loss: 0.1425 - image_quality_output_loss: 0.3330 - age_output_loss: 0.3801 - weight_output_loss: 0.3099 - bag_output_loss: 0.2883 - footwear_output_loss: 0.2756 - pose_output_loss: 0.1836 - emotion_output_loss: 0.2937 - gender_output_acc: 0.9434 - image_quality_output_acc: 0.8671 - age_output_acc: 0.8614 - weight_output_acc: 0.8865 - bag_output_acc: 0.8872 - footwear_output_acc: 0.8933 - pose_output_acc: 0.9339 - emotion_output_acc: 0.9009 - val_loss: 10.9913 - val_gender_output_loss: 0.5046 - val_image_quality_output_loss: 1.4231 - val_age_output_loss: 2.3367 - val_weight_output_loss: 1.4948 - val_bag_output_loss: 1.2810 - val_footwear_output_loss: 1.0968 - val_pose_output_loss: 0.7364 - val_emotion_output_loss: 1.4169 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 7.24353\n",
            "Epoch 47/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3179 - gender_output_loss: 0.1445 - image_quality_output_loss: 0.3304 - age_output_loss: 0.3820 - weight_output_loss: 0.3126 - bag_output_loss: 0.2808 - footwear_output_loss: 0.2785 - pose_output_loss: 0.1847 - emotion_output_loss: 0.2897 - gender_output_acc: 0.9412 - image_quality_output_acc: 0.8681 - age_output_acc: 0.8586 - weight_output_acc: 0.8903 - bag_output_acc: 0.8924 - footwear_output_acc: 0.8958 - pose_output_acc: 0.9326 - emotion_output_acc: 0.9022 - val_loss: 11.0155 - val_gender_output_loss: 0.5069 - val_image_quality_output_loss: 1.4235 - val_age_output_loss: 2.3372 - val_weight_output_loss: 1.4959 - val_bag_output_loss: 1.2868 - val_footwear_output_loss: 1.1050 - val_pose_output_loss: 0.7383 - val_emotion_output_loss: 1.4207 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5595 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7807 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 7.24353\n",
            "Epoch 48/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3261 - gender_output_loss: 0.1491 - image_quality_output_loss: 0.3338 - age_output_loss: 0.3763 - weight_output_loss: 0.3116 - bag_output_loss: 0.2831 - footwear_output_loss: 0.2747 - pose_output_loss: 0.1868 - emotion_output_loss: 0.2979 - gender_output_acc: 0.9384 - image_quality_output_acc: 0.8661 - age_output_acc: 0.8607 - weight_output_acc: 0.8872 - bag_output_acc: 0.8921 - footwear_output_acc: 0.8983 - pose_output_acc: 0.9312 - emotion_output_acc: 0.8999 - val_loss: 11.1208 - val_gender_output_loss: 0.5096 - val_image_quality_output_loss: 1.4379 - val_age_output_loss: 2.3678 - val_weight_output_loss: 1.5110 - val_bag_output_loss: 1.2938 - val_footwear_output_loss: 1.1118 - val_pose_output_loss: 0.7451 - val_emotion_output_loss: 1.4334 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5207 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 7.24353\n",
            "Epoch 49/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.4393 - gender_output_loss: 0.1543 - image_quality_output_loss: 0.3493 - age_output_loss: 0.3988 - weight_output_loss: 0.3208 - bag_output_loss: 0.2959 - footwear_output_loss: 0.2939 - pose_output_loss: 0.2004 - emotion_output_loss: 0.3064 - gender_output_acc: 0.9365 - image_quality_output_acc: 0.8578 - age_output_acc: 0.8526 - weight_output_acc: 0.8825 - bag_output_acc: 0.8852 - footwear_output_acc: 0.8886 - pose_output_acc: 0.9254 - emotion_output_acc: 0.8998 - val_loss: 11.0571 - val_gender_output_loss: 0.5072 - val_image_quality_output_loss: 1.4323 - val_age_output_loss: 2.3473 - val_weight_output_loss: 1.5029 - val_bag_output_loss: 1.2852 - val_footwear_output_loss: 1.1068 - val_pose_output_loss: 0.7435 - val_emotion_output_loss: 1.4277 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5640 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 7.24353\n",
            "Epoch 50/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3274 - gender_output_loss: 0.1471 - image_quality_output_loss: 0.3428 - age_output_loss: 0.3746 - weight_output_loss: 0.3189 - bag_output_loss: 0.2794 - footwear_output_loss: 0.2757 - pose_output_loss: 0.1906 - emotion_output_loss: 0.2859 - gender_output_acc: 0.9399 - image_quality_output_acc: 0.8647 - age_output_acc: 0.8617 - weight_output_acc: 0.8813 - bag_output_acc: 0.8916 - footwear_output_acc: 0.8926 - pose_output_acc: 0.9305 - emotion_output_acc: 0.9021Learning rate:  5e-07\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3272 - gender_output_loss: 0.1471 - image_quality_output_loss: 0.3428 - age_output_loss: 0.3741 - weight_output_loss: 0.3188 - bag_output_loss: 0.2796 - footwear_output_loss: 0.2755 - pose_output_loss: 0.1907 - emotion_output_loss: 0.2863 - gender_output_acc: 0.9399 - image_quality_output_acc: 0.8648 - age_output_acc: 0.8617 - weight_output_acc: 0.8813 - bag_output_acc: 0.8914 - footwear_output_acc: 0.8927 - pose_output_acc: 0.9302 - emotion_output_acc: 0.9020 - val_loss: 11.0254 - val_gender_output_loss: 0.5033 - val_image_quality_output_loss: 1.4293 - val_age_output_loss: 2.3425 - val_weight_output_loss: 1.4973 - val_bag_output_loss: 1.2873 - val_footwear_output_loss: 1.0994 - val_pose_output_loss: 0.7394 - val_emotion_output_loss: 1.4244 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6462 - val_pose_output_acc: 0.7802 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 7.24353\n",
            "Epoch 51/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3266 - gender_output_loss: 0.1515 - image_quality_output_loss: 0.3334 - age_output_loss: 0.3774 - weight_output_loss: 0.3155 - bag_output_loss: 0.2750 - footwear_output_loss: 0.2809 - pose_output_loss: 0.1842 - emotion_output_loss: 0.2954 - gender_output_acc: 0.9396 - image_quality_output_acc: 0.8675 - age_output_acc: 0.8628 - weight_output_acc: 0.8869 - bag_output_acc: 0.8950 - footwear_output_acc: 0.8897 - pose_output_acc: 0.9295 - emotion_output_acc: 0.8987 - val_loss: 11.0852 - val_gender_output_loss: 0.5080 - val_image_quality_output_loss: 1.4355 - val_age_output_loss: 2.3584 - val_weight_output_loss: 1.5023 - val_bag_output_loss: 1.2911 - val_footwear_output_loss: 1.1064 - val_pose_output_loss: 0.7445 - val_emotion_output_loss: 1.4314 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 7.24353\n",
            "Epoch 52/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3329 - gender_output_loss: 0.1475 - image_quality_output_loss: 0.3274 - age_output_loss: 0.3875 - weight_output_loss: 0.3155 - bag_output_loss: 0.2738 - footwear_output_loss: 0.2849 - pose_output_loss: 0.1873 - emotion_output_loss: 0.2928 - gender_output_acc: 0.9374 - image_quality_output_acc: 0.8706 - age_output_acc: 0.8535 - weight_output_acc: 0.8823 - bag_output_acc: 0.8922 - footwear_output_acc: 0.8909 - pose_output_acc: 0.9298 - emotion_output_acc: 0.9006 - val_loss: 11.0943 - val_gender_output_loss: 0.5099 - val_image_quality_output_loss: 1.4337 - val_age_output_loss: 2.3575 - val_weight_output_loss: 1.5044 - val_bag_output_loss: 1.2963 - val_footwear_output_loss: 1.1079 - val_pose_output_loss: 0.7456 - val_emotion_output_loss: 1.4318 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5222 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7802 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 7.24353\n",
            "Epoch 53/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3930 - gender_output_loss: 0.1611 - image_quality_output_loss: 0.3438 - age_output_loss: 0.3870 - weight_output_loss: 0.3131 - bag_output_loss: 0.2923 - footwear_output_loss: 0.2872 - pose_output_loss: 0.1961 - emotion_output_loss: 0.2964 - gender_output_acc: 0.9333 - image_quality_output_acc: 0.8630 - age_output_acc: 0.8614 - weight_output_acc: 0.8878 - bag_output_acc: 0.8886 - footwear_output_acc: 0.8880 - pose_output_acc: 0.9275 - emotion_output_acc: 0.8994 - val_loss: 11.0189 - val_gender_output_loss: 0.5057 - val_image_quality_output_loss: 1.4274 - val_age_output_loss: 2.3337 - val_weight_output_loss: 1.5012 - val_bag_output_loss: 1.2825 - val_footwear_output_loss: 1.1051 - val_pose_output_loss: 0.7411 - val_emotion_output_loss: 1.4221 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.3589 - val_weight_output_acc: 0.5615 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 7.24353\n",
            "Epoch 54/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.4177 - gender_output_loss: 0.1542 - image_quality_output_loss: 0.3489 - age_output_loss: 0.3918 - weight_output_loss: 0.3250 - bag_output_loss: 0.2903 - footwear_output_loss: 0.2834 - pose_output_loss: 0.1965 - emotion_output_loss: 0.3099 - gender_output_acc: 0.9356 - image_quality_output_acc: 0.8579 - age_output_acc: 0.8575 - weight_output_acc: 0.8788 - bag_output_acc: 0.8872 - footwear_output_acc: 0.8920 - pose_output_acc: 0.9267 - emotion_output_acc: 0.8943\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 5e-07.\n",
            "Epoch 00053: val_loss did not improve from 7.24353\n",
            "Epoch 54/100\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.4191 - gender_output_loss: 0.1546 - image_quality_output_loss: 0.3487 - age_output_loss: 0.3917 - weight_output_loss: 0.3255 - bag_output_loss: 0.2903 - footwear_output_loss: 0.2832 - pose_output_loss: 0.1968 - emotion_output_loss: 0.3107 - gender_output_acc: 0.9353 - image_quality_output_acc: 0.8580 - age_output_acc: 0.8576 - weight_output_acc: 0.8786 - bag_output_acc: 0.8871 - footwear_output_acc: 0.8922 - pose_output_acc: 0.9264 - emotion_output_acc: 0.8939 - val_loss: 11.1208 - val_gender_output_loss: 0.5106 - val_image_quality_output_loss: 1.4395 - val_age_output_loss: 2.3638 - val_weight_output_loss: 1.5089 - val_bag_output_loss: 1.3017 - val_footwear_output_loss: 1.1066 - val_pose_output_loss: 0.7472 - val_emotion_output_loss: 1.4334 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3513 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 7.24353\n",
            "Epoch 55/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.4274 - gender_output_loss: 0.1576 - image_quality_output_loss: 0.3468 - age_output_loss: 0.3942 - weight_output_loss: 0.3293 - bag_output_loss: 0.2880 - footwear_output_loss: 0.2840 - pose_output_loss: 0.2022 - emotion_output_loss: 0.3071 - gender_output_acc: 0.9347 - image_quality_output_acc: 0.8615 - age_output_acc: 0.8525 - weight_output_acc: 0.8814 - bag_output_acc: 0.8864 - footwear_output_acc: 0.8897 - pose_output_acc: 0.9237 - emotion_output_acc: 0.8970 - val_loss: 11.0004 - val_gender_output_loss: 0.5082 - val_image_quality_output_loss: 1.4203 - val_age_output_loss: 2.3336 - val_weight_output_loss: 1.4868 - val_bag_output_loss: 1.2850 - val_footwear_output_loss: 1.1059 - val_pose_output_loss: 0.7394 - val_emotion_output_loss: 1.4212 - val_gender_output_acc: 0.8241 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3533 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 7.24353\n",
            "Epoch 56/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3523 - gender_output_loss: 0.1538 - image_quality_output_loss: 0.3410 - age_output_loss: 0.3748 - weight_output_loss: 0.3208 - bag_output_loss: 0.2848 - footwear_output_loss: 0.2774 - pose_output_loss: 0.1920 - emotion_output_loss: 0.2953 - gender_output_acc: 0.9360 - image_quality_output_acc: 0.8637 - age_output_acc: 0.8631 - weight_output_acc: 0.8844 - bag_output_acc: 0.8911 - footwear_output_acc: 0.8914 - pose_output_acc: 0.9279 - emotion_output_acc: 0.9008\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3530 - gender_output_loss: 0.1541 - image_quality_output_loss: 0.3413 - age_output_loss: 0.3747 - weight_output_loss: 0.3210 - bag_output_loss: 0.2848 - footwear_output_loss: 0.2774 - pose_output_loss: 0.1917 - emotion_output_loss: 0.2955 - gender_output_acc: 0.9358 - image_quality_output_acc: 0.8635 - age_output_acc: 0.8630 - weight_output_acc: 0.8843 - bag_output_acc: 0.8911 - footwear_output_acc: 0.8913 - pose_output_acc: 0.9280 - emotion_output_acc: 0.9006 - val_loss: 11.0599 - val_gender_output_loss: 0.5066 - val_image_quality_output_loss: 1.4304 - val_age_output_loss: 2.3563 - val_weight_output_loss: 1.5008 - val_bag_output_loss: 1.2901 - val_footwear_output_loss: 1.1041 - val_pose_output_loss: 0.7389 - val_emotion_output_loss: 1.4258 - val_gender_output_acc: 0.8246 - val_image_quality_output_acc: 0.5237 - val_age_output_acc: 0.3569 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7823 - val_emotion_output_acc: 0.6416\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 7.24353\n",
            "Epoch 57/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3004 - gender_output_loss: 0.1448 - image_quality_output_loss: 0.3317 - age_output_loss: 0.3726 - weight_output_loss: 0.3112 - bag_output_loss: 0.2790 - footwear_output_loss: 0.2716 - pose_output_loss: 0.1806 - emotion_output_loss: 0.2971 - gender_output_acc: 0.9398 - image_quality_output_acc: 0.8696 - age_output_acc: 0.8628 - weight_output_acc: 0.8871 - bag_output_acc: 0.8930 - footwear_output_acc: 0.8960 - pose_output_acc: 0.9339 - emotion_output_acc: 0.9025 - val_loss: 11.0072 - val_gender_output_loss: 0.5066 - val_image_quality_output_loss: 1.4256 - val_age_output_loss: 2.3355 - val_weight_output_loss: 1.4996 - val_bag_output_loss: 1.2841 - val_footwear_output_loss: 1.0962 - val_pose_output_loss: 0.7401 - val_emotion_output_loss: 1.4189 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6396\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 7.24353\n",
            "Epoch 58/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3004 - gender_output_loss: 0.1448 - image_quality_output_loss: 0.3317 - age_output_loss: 0.3726 - weight_output_loss: 0.3112 - bag_output_loss: 0.2790 - footwear_output_loss: 0.2716 - pose_output_loss: 0.1806 - emotion_output_loss: 0.2971 - gender_output_acc: 0.9398 - image_quality_output_acc: 0.8696 - age_output_acc: 0.8628 - weight_output_acc: 0.8871 - bag_output_acc: 0.8930 - footwear_output_acc: 0.8960 - pose_output_acc: 0.9339 - emotion_output_acc: 0.9025 - val_loss: 11.0072 - val_gender_output_loss: 0.5066 - val_image_quality_output_loss: 1.4256 - val_age_output_loss: 2.3355 - val_weight_output_loss: 1.4996 - val_bag_output_loss: 1.2841 - val_footwear_output_loss: 1.0962 - val_pose_output_loss: 0.7401 - val_emotion_output_loss: 1.4189 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6396\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.2966 - gender_output_loss: 0.1460 - image_quality_output_loss: 0.3277 - age_output_loss: 0.3660 - weight_output_loss: 0.3145 - bag_output_loss: 0.2820 - footwear_output_loss: 0.2812 - pose_output_loss: 0.1852 - emotion_output_loss: 0.2841 - gender_output_acc: 0.9382 - image_quality_output_acc: 0.8740 - age_output_acc: 0.8683 - weight_output_acc: 0.8872 - bag_output_acc: 0.8904 - footwear_output_acc: 0.8900 - pose_output_acc: 0.9322 - emotion_output_acc: 0.9016 - val_loss: 10.9999 - val_gender_output_loss: 0.5045 - val_image_quality_output_loss: 1.4236 - val_age_output_loss: 2.3338 - val_weight_output_loss: 1.4951 - val_bag_output_loss: 1.2802 - val_footwear_output_loss: 1.1012 - val_pose_output_loss: 0.7390 - val_emotion_output_loss: 1.4224 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 7.24353\n",
            "Epoch 59/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3148 - gender_output_loss: 0.1465 - image_quality_output_loss: 0.3332 - age_output_loss: 0.3729 - weight_output_loss: 0.3097 - bag_output_loss: 0.2861 - footwear_output_loss: 0.2752 - pose_output_loss: 0.1886 - emotion_output_loss: 0.2908 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.8676 - age_output_acc: 0.8653 - weight_output_acc: 0.8846 - bag_output_acc: 0.8878 - footwear_output_acc: 0.8951 - pose_output_acc: 0.9317 - emotion_output_acc: 0.9018 - val_loss: 11.0143 - val_gender_output_loss: 0.5073 - val_image_quality_output_loss: 1.4267 - val_age_output_loss: 2.3349 - val_weight_output_loss: 1.4981 - val_bag_output_loss: 1.2852 - val_footwear_output_loss: 1.0970 - val_pose_output_loss: 0.7418 - val_emotion_output_loss: 1.4229 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5605 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 7.24353\n",
            "Epoch 60/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.2928 - gender_output_loss: 0.1474 - image_quality_output_loss: 0.3306 - age_output_loss: 0.3688 - weight_output_loss: 0.3075 - bag_output_loss: 0.2790 - footwear_output_loss: 0.2726 - pose_output_loss: 0.1850 - emotion_output_loss: 0.2913 - gender_output_acc: 0.9392 - image_quality_output_acc: 0.8692 - age_output_acc: 0.8647 - weight_output_acc: 0.8927 - bag_output_acc: 0.8911 - footwear_output_acc: 0.8946 - pose_output_acc: 0.9325 - emotion_output_acc: 0.8990 - val_loss: 11.0312 - val_gender_output_loss: 0.5063 - val_image_quality_output_loss: 1.4263 - val_age_output_loss: 2.3413 - val_weight_output_loss: 1.4954 - val_bag_output_loss: 1.2886 - val_footwear_output_loss: 1.1064 - val_pose_output_loss: 0.7421 - val_emotion_output_loss: 1.4225 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5227 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 7.24353\n",
            "Epoch 61/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.2593 - gender_output_loss: 0.1426 - image_quality_output_loss: 0.3215 - age_output_loss: 0.3723 - weight_output_loss: 0.3017 - bag_output_loss: 0.2707 - footwear_output_loss: 0.2722 - pose_output_loss: 0.1754 - emotion_output_loss: 0.2913 - gender_output_acc: 0.9393 - image_quality_output_acc: 0.8714 - age_output_acc: 0.8627 - weight_output_acc: 0.8921 - bag_output_acc: 0.8975 - footwear_output_acc: 0.8955 - pose_output_acc: 0.9355 - emotion_output_acc: 0.9015 5e-07\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.2583 - gender_output_loss: 0.1425 - image_quality_output_loss: 0.3215 - age_output_loss: 0.3722 - weight_output_loss: 0.3014 - bag_output_loss: 0.2705 - footwear_output_loss: 0.2724 - pose_output_loss: 0.1752 - emotion_output_loss: 0.2911 - gender_output_acc: 0.9394 - image_quality_output_acc: 0.8714 - age_output_acc: 0.8628 - weight_output_acc: 0.8922 - bag_output_acc: 0.8975 - footwear_output_acc: 0.8953 - pose_output_acc: 0.9356 - emotion_output_acc: 0.9017 - val_loss: 10.9773 - val_gender_output_loss: 0.5066 - val_image_quality_output_loss: 1.4206 - val_age_output_loss: 2.3285 - val_weight_output_loss: 1.4899 - val_bag_output_loss: 1.2855 - val_footwear_output_loss: 1.0948 - val_pose_output_loss: 0.7381 - val_emotion_output_loss: 1.4147 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 7.24353\n",
            "Epoch 62/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.2334 - gender_output_loss: 0.1402 - image_quality_output_loss: 0.3222 - age_output_loss: 0.3655 - weight_output_loss: 0.2973 - bag_output_loss: 0.2660 - footwear_output_loss: 0.2705 - pose_output_loss: 0.1780 - emotion_output_loss: 0.2841 - gender_output_acc: 0.9438 - image_quality_output_acc: 0.8757 - age_output_acc: 0.8682 - weight_output_acc: 0.8948 - bag_output_acc: 0.8996 - footwear_output_acc: 0.8974 - pose_output_acc: 0.9336 - emotion_output_acc: 0.9064 - val_loss: 11.0008 - val_gender_output_loss: 0.5057 - val_image_quality_output_loss: 1.4203 - val_age_output_loss: 2.3388 - val_weight_output_loss: 1.4961 - val_bag_output_loss: 1.2816 - val_footwear_output_loss: 1.0977 - val_pose_output_loss: 0.7385 - val_emotion_output_loss: 1.4205 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6356\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 7.24353\n",
            "Epoch 63/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3083 - gender_output_loss: 0.1455 - image_quality_output_loss: 0.3226 - age_output_loss: 0.3756 - weight_output_loss: 0.3115 - bag_output_loss: 0.2815 - footwear_output_loss: 0.2667 - pose_output_loss: 0.1991 - emotion_output_loss: 0.2931 - gender_output_acc: 0.9366 - image_quality_output_acc: 0.8749 - age_output_acc: 0.8639 - weight_output_acc: 0.8894 - bag_output_acc: 0.8901 - footwear_output_acc: 0.9000 - pose_output_acc: 0.9279 - emotion_output_acc: 0.9007 - val_loss: 11.0017 - val_gender_output_loss: 0.5053 - val_image_quality_output_loss: 1.4232 - val_age_output_loss: 2.3360 - val_weight_output_loss: 1.4924 - val_bag_output_loss: 1.2841 - val_footwear_output_loss: 1.0998 - val_pose_output_loss: 0.7373 - val_emotion_output_loss: 1.4229 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6416\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 7.24353\n",
            "Epoch 64/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.3375 - gender_output_loss: 0.1511 - image_quality_output_loss: 0.3334 - age_output_loss: 0.3764 - weight_output_loss: 0.3214 - bag_output_loss: 0.2766 - footwear_output_loss: 0.2848 - pose_output_loss: 0.1898 - emotion_output_loss: 0.2911 - gender_output_acc: 0.9371 - image_quality_output_acc: 0.8691 - age_output_acc: 0.8597 - weight_output_acc: 0.8867 - bag_output_acc: 0.8933 - footwear_output_acc: 0.8903 - pose_output_acc: 0.9281 - emotion_output_acc: 0.8990 - val_loss: 11.0427 - val_gender_output_loss: 0.5077 - val_image_quality_output_loss: 1.4299 - val_age_output_loss: 2.3430 - val_weight_output_loss: 1.4984 - val_bag_output_loss: 1.2883 - val_footwear_output_loss: 1.1025 - val_pose_output_loss: 0.7430 - val_emotion_output_loss: 1.4269 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5237 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 7.24353\n",
            "Epoch 65/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3299 - gender_output_loss: 0.1460 - image_quality_output_loss: 0.3349 - age_output_loss: 0.3822 - weight_output_loss: 0.3130 - bag_output_loss: 0.2777 - footwear_output_loss: 0.2776 - pose_output_loss: 0.1927 - emotion_output_loss: 0.2911 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.8676 - age_output_acc: 0.8597 - weight_output_acc: 0.8852 - bag_output_acc: 0.8902 - footwear_output_acc: 0.8946 - pose_output_acc: 0.9280 - emotion_output_acc: 0.9019 - val_loss: 11.1097 - val_gender_output_loss: 0.5102 - val_image_quality_output_loss: 1.4373 - val_age_output_loss: 2.3562 - val_weight_output_loss: 1.5141 - val_bag_output_loss: 1.2977 - val_footwear_output_loss: 1.1086 - val_pose_output_loss: 0.7463 - val_emotion_output_loss: 1.4322 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7807 - val_emotion_output_acc: 0.6346\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 7.24353\n",
            "Epoch 66/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3409 - gender_output_loss: 0.1507 - image_quality_output_loss: 0.3311 - age_output_loss: 0.3808 - weight_output_loss: 0.3117 - bag_output_loss: 0.2798 - footwear_output_loss: 0.2898 - pose_output_loss: 0.1875 - emotion_output_loss: 0.2952 - gender_output_acc: 0.9369 - image_quality_output_acc: 0.8700 - age_output_acc: 0.8585 - weight_output_acc: 0.8878 - bag_output_acc: 0.8921 - footwear_output_acc: 0.8894 - pose_output_acc: 0.9264 - emotion_output_acc: 0.8992\n",
            "Epoch 00065: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3422 - gender_output_loss: 0.1506 - image_quality_output_loss: 0.3311 - age_output_loss: 0.3815 - weight_output_loss: 0.3119 - bag_output_loss: 0.2799 - footwear_output_loss: 0.2898 - pose_output_loss: 0.1878 - emotion_output_loss: 0.2952 - gender_output_acc: 0.9370 - image_quality_output_acc: 0.8701 - age_output_acc: 0.8582 - weight_output_acc: 0.8878 - bag_output_acc: 0.8920 - footwear_output_acc: 0.8894 - pose_output_acc: 0.9263 - emotion_output_acc: 0.8993 - val_loss: 11.0521 - val_gender_output_loss: 0.5088 - val_image_quality_output_loss: 1.4291 - val_age_output_loss: 2.3444 - val_weight_output_loss: 1.4983 - val_bag_output_loss: 1.2885 - val_footwear_output_loss: 1.1031 - val_pose_output_loss: 0.7439 - val_emotion_output_loss: 1.4325 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5272 - val_age_output_acc: 0.3569 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7818 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 7.24353\n",
            "Epoch 67/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.4137 - gender_output_loss: 0.1571 - image_quality_output_loss: 0.3387 - age_output_loss: 0.3963 - weight_output_loss: 0.3172 - bag_output_loss: 0.2984 - footwear_output_loss: 0.2851 - pose_output_loss: 0.1942 - emotion_output_loss: 0.3079 - gender_output_acc: 0.9342 - image_quality_output_acc: 0.8639 - age_output_acc: 0.8565 - weight_output_acc: 0.8856 - bag_output_acc: 0.8825 - footwear_output_acc: 0.8909 - pose_output_acc: 0.9242 - emotion_output_acc: 0.8959 - val_loss: 11.1055 - val_gender_output_loss: 0.5076 - val_image_quality_output_loss: 1.4384 - val_age_output_loss: 2.3605 - val_weight_output_loss: 1.5070 - val_bag_output_loss: 1.2926 - val_footwear_output_loss: 1.1104 - val_pose_output_loss: 0.7462 - val_emotion_output_loss: 1.4346 - val_gender_output_acc: 0.8276 - val_image_quality_output_acc: 0.5227 - val_age_output_acc: 0.3604 - val_weight_output_acc: 0.5630 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 7.24353\n",
            "Epoch 68/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3880 - gender_output_loss: 0.1530 - image_quality_output_loss: 0.3416 - age_output_loss: 0.3923 - weight_output_loss: 0.3224 - bag_output_loss: 0.2805 - footwear_output_loss: 0.2862 - pose_output_loss: 0.1920 - emotion_output_loss: 0.3024 - gender_output_acc: 0.9375 - image_quality_output_acc: 0.8639 - age_output_acc: 0.8558 - weight_output_acc: 0.8841 - bag_output_acc: 0.8901 - footwear_output_acc: 0.8895 - pose_output_acc: 0.9264 - emotion_output_acc: 0.8964\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 5e-07.\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3868 - gender_output_loss: 0.1529 - image_quality_output_loss: 0.3414 - age_output_loss: 0.3925 - weight_output_loss: 0.3223 - bag_output_loss: 0.2802 - footwear_output_loss: 0.2859 - pose_output_loss: 0.1918 - emotion_output_loss: 0.3020 - gender_output_acc: 0.9375 - image_quality_output_acc: 0.8639 - age_output_acc: 0.8557 - weight_output_acc: 0.8844 - bag_output_acc: 0.8902 - footwear_output_acc: 0.8897 - pose_output_acc: 0.9264 - emotion_output_acc: 0.8966 - val_loss: 11.0289 - val_gender_output_loss: 0.5068 - val_image_quality_output_loss: 1.4307 - val_age_output_loss: 2.3425 - val_weight_output_loss: 1.4961 - val_bag_output_loss: 1.2842 - val_footwear_output_loss: 1.1014 - val_pose_output_loss: 0.7406 - val_emotion_output_loss: 1.4237 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 7.24353\n",
            "Epoch 69/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3982 - gender_output_loss: 0.1496 - image_quality_output_loss: 0.3348 - age_output_loss: 0.3887 - weight_output_loss: 0.3259 - bag_output_loss: 0.2946 - footwear_output_loss: 0.2913 - pose_output_loss: 0.1963 - emotion_output_loss: 0.3004 - gender_output_acc: 0.9372 - image_quality_output_acc: 0.8664 - age_output_acc: 0.8537 - weight_output_acc: 0.8798 - bag_output_acc: 0.8854 - footwear_output_acc: 0.8857 - pose_output_acc: 0.9256 - emotion_output_acc: 0.8975 - val_loss: 11.0462 - val_gender_output_loss: 0.5062 - val_image_quality_output_loss: 1.4308 - val_age_output_loss: 2.3441 - val_weight_output_loss: 1.4995 - val_bag_output_loss: 1.2901 - val_footwear_output_loss: 1.1011 - val_pose_output_loss: 0.7401 - val_emotion_output_loss: 1.4311 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5237 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6462 - val_pose_output_acc: 0.7802 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 7.24353\n",
            "Epoch 70/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.4034 - gender_output_loss: 0.1570 - image_quality_output_loss: 0.3471 - age_output_loss: 0.3927 - weight_output_loss: 0.3224 - bag_output_loss: 0.2930 - footwear_output_loss: 0.2829 - pose_output_loss: 0.1956 - emotion_output_loss: 0.2949 - gender_output_acc: 0.9361 - image_quality_output_acc: 0.8630 - age_output_acc: 0.8553 - weight_output_acc: 0.8821 - bag_output_acc: 0.8874 - footwear_output_acc: 0.8902 - pose_output_acc: 0.9260 - emotion_output_acc: 0.8962 - val_loss: 11.1342 - val_gender_output_loss: 0.5098 - val_image_quality_output_loss: 1.4397 - val_age_output_loss: 2.3698 - val_weight_output_loss: 1.5113 - val_bag_output_loss: 1.2983 - val_footwear_output_loss: 1.1126 - val_pose_output_loss: 0.7450 - val_emotion_output_loss: 1.4367 - val_gender_output_acc: 0.8291 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 7.24353\n",
            "Epoch 71/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3630 - gender_output_loss: 0.1524 - image_quality_output_loss: 0.3353 - age_output_loss: 0.3848 - weight_output_loss: 0.3181 - bag_output_loss: 0.2899 - footwear_output_loss: 0.2765 - pose_output_loss: 0.1949 - emotion_output_loss: 0.2957 - gender_output_acc: 0.9384 - image_quality_output_acc: 0.8707 - age_output_acc: 0.8551 - weight_output_acc: 0.8824 - bag_output_acc: 0.8883 - footwear_output_acc: 0.8926 - pose_output_acc: 0.9262 - emotion_output_acc: 0.9029 - val_loss: 11.0341 - val_gender_output_loss: 0.5082 - val_image_quality_output_loss: 1.4290 - val_age_output_loss: 2.3436 - val_weight_output_loss: 1.4950 - val_bag_output_loss: 1.2916 - val_footwear_output_loss: 1.0986 - val_pose_output_loss: 0.7412 - val_emotion_output_loss: 1.4240 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3518 - val_weight_output_acc: 0.5615 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 7.24353\n",
            "Epoch 72/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.2625 - gender_output_loss: 0.1427 - image_quality_output_loss: 0.3271 - age_output_loss: 0.3692 - weight_output_loss: 0.3041 - bag_output_loss: 0.2803 - footwear_output_loss: 0.2706 - pose_output_loss: 0.1781 - emotion_output_loss: 0.2794 - gender_output_acc: 0.9414 - image_quality_output_acc: 0.8707 - age_output_acc: 0.8628 - weight_output_acc: 0.8912 - bag_output_acc: 0.8946 - footwear_output_acc: 0.8981 - pose_output_acc: 0.9338 - emotion_output_acc: 0.9075 - val_loss: 10.9832 - val_gender_output_loss: 0.5044 - val_image_quality_output_loss: 1.4197 - val_age_output_loss: 2.3305 - val_weight_output_loss: 1.4939 - val_bag_output_loss: 1.2801 - val_footwear_output_loss: 1.0965 - val_pose_output_loss: 0.7386 - val_emotion_output_loss: 1.4204 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.3569 - val_weight_output_acc: 0.5595 - val_bag_output_acc: 0.5847 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6396\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 7.24353\n",
            "Epoch 73/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.2611 - gender_output_loss: 0.1445 - image_quality_output_loss: 0.3265 - age_output_loss: 0.3658 - weight_output_loss: 0.3007 - bag_output_loss: 0.2741 - footwear_output_loss: 0.2736 - pose_output_loss: 0.1854 - emotion_output_loss: 0.2809 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.8707 - age_output_acc: 0.8653 - weight_output_acc: 0.8898 - bag_output_acc: 0.8972 - footwear_output_acc: 0.8964 - pose_output_acc: 0.9321 - emotion_output_acc: 0.9048 - val_loss: 11.0463 - val_gender_output_loss: 0.5067 - val_image_quality_output_loss: 1.4284 - val_age_output_loss: 2.3506 - val_weight_output_loss: 1.4991 - val_bag_output_loss: 1.2877 - val_footwear_output_loss: 1.1058 - val_pose_output_loss: 0.7391 - val_emotion_output_loss: 1.4237 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5635 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6477 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 7.24353\n",
            "Epoch 74/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.2450 - gender_output_loss: 0.1453 - image_quality_output_loss: 0.3302 - age_output_loss: 0.3657 - weight_output_loss: 0.2966 - bag_output_loss: 0.2728 - footwear_output_loss: 0.2699 - pose_output_loss: 0.1738 - emotion_output_loss: 0.2810 - gender_output_acc: 0.9401 - image_quality_output_acc: 0.8760 - age_output_acc: 0.8625 - weight_output_acc: 0.8923 - bag_output_acc: 0.8962 - footwear_output_acc: 0.8944 - pose_output_acc: 0.9354 - emotion_output_acc: 0.9054 - val_loss: 10.9678 - val_gender_output_loss: 0.5036 - val_image_quality_output_loss: 1.4220 - val_age_output_loss: 2.3262 - val_weight_output_loss: 1.4874 - val_bag_output_loss: 1.2820 - val_footwear_output_loss: 1.0960 - val_pose_output_loss: 0.7363 - val_emotion_output_loss: 1.4165 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3548 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7747 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 7.24353\n",
            "Epoch 75/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.2827 - gender_output_loss: 0.1546 - image_quality_output_loss: 0.3232 - age_output_loss: 0.3728 - weight_output_loss: 0.3104 - bag_output_loss: 0.2721 - footwear_output_loss: 0.2646 - pose_output_loss: 0.1841 - emotion_output_loss: 0.2889 - gender_output_acc: 0.9359 - image_quality_output_acc: 0.8744 - age_output_acc: 0.8619 - weight_output_acc: 0.8902 - bag_output_acc: 0.8952 - footwear_output_acc: 0.8997 - pose_output_acc: 0.9302 - emotion_output_acc: 0.9016\n",
            "Epoch 75/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.2830 - gender_output_loss: 0.1548 - image_quality_output_loss: 0.3235 - age_output_loss: 0.3728 - weight_output_loss: 0.3104 - bag_output_loss: 0.2718 - footwear_output_loss: 0.2647 - pose_output_loss: 0.1844 - emotion_output_loss: 0.2888 - gender_output_acc: 0.9359 - image_quality_output_acc: 0.8740 - age_output_acc: 0.8617 - weight_output_acc: 0.8902 - bag_output_acc: 0.8954 - footwear_output_acc: 0.8996 - pose_output_acc: 0.9300 - emotion_output_acc: 0.9018 - val_loss: 11.0625 - val_gender_output_loss: 0.5050 - val_image_quality_output_loss: 1.4315 - val_age_output_loss: 2.3480 - val_weight_output_loss: 1.5040 - val_bag_output_loss: 1.2887 - val_footwear_output_loss: 1.1063 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 1.4322 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 7.24353\n",
            "Epoch 76/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.3287 - gender_output_loss: 0.1461 - image_quality_output_loss: 0.3402 - age_output_loss: 0.3802 - weight_output_loss: 0.3130 - bag_output_loss: 0.2827 - footwear_output_loss: 0.2713 - pose_output_loss: 0.1860 - emotion_output_loss: 0.2950 - gender_output_acc: 0.9404 - image_quality_output_acc: 0.8636 - age_output_acc: 0.8617 - weight_output_acc: 0.8901 - bag_output_acc: 0.8916 - footwear_output_acc: 0.8984 - pose_output_acc: 0.9321 - emotion_output_acc: 0.8984 - val_loss: 11.0485 - val_gender_output_loss: 0.5057 - val_image_quality_output_loss: 1.4297 - val_age_output_loss: 2.3415 - val_weight_output_loss: 1.5036 - val_bag_output_loss: 1.2890 - val_footwear_output_loss: 1.1038 - val_pose_output_loss: 0.7421 - val_emotion_output_loss: 1.4308 - val_gender_output_acc: 0.8281 - val_image_quality_output_acc: 0.5222 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 7.24353\n",
            "Epoch 77/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.2940 - gender_output_loss: 0.1431 - image_quality_output_loss: 0.3258 - age_output_loss: 0.3702 - weight_output_loss: 0.3123 - bag_output_loss: 0.2819 - footwear_output_loss: 0.2820 - pose_output_loss: 0.1814 - emotion_output_loss: 0.2862 - gender_output_acc: 0.9425 - image_quality_output_acc: 0.8709 - age_output_acc: 0.8629 - weight_output_acc: 0.8845 - bag_output_acc: 0.8911 - footwear_output_acc: 0.8938 - pose_output_acc: 0.9309 - emotion_output_acc: 0.9058\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.2941 - gender_output_loss: 0.1429 - image_quality_output_loss: 0.3254 - age_output_loss: 0.3701 - weight_output_loss: 0.3122 - bag_output_loss: 0.2823 - footwear_output_loss: 0.2820 - pose_output_loss: 0.1817 - emotion_output_loss: 0.2864 - gender_output_acc: 0.9427 - image_quality_output_acc: 0.8710 - age_output_acc: 0.8629 - weight_output_acc: 0.8845 - bag_output_acc: 0.8910 - footwear_output_acc: 0.8937 - pose_output_acc: 0.9309 - emotion_output_acc: 0.9057 - val_loss: 11.0010 - val_gender_output_loss: 0.5053 - val_image_quality_output_loss: 1.4282 - val_age_output_loss: 2.3351 - val_weight_output_loss: 1.4896 - val_bag_output_loss: 1.2872 - val_footwear_output_loss: 1.0972 - val_pose_output_loss: 0.7389 - val_emotion_output_loss: 1.4189 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 7.24353\n",
            "Epoch 78/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.2503 - gender_output_loss: 0.1408 - image_quality_output_loss: 0.3215 - age_output_loss: 0.3641 - weight_output_loss: 0.3065 - bag_output_loss: 0.2757 - footwear_output_loss: 0.2673 - pose_output_loss: 0.1852 - emotion_output_loss: 0.2799 - gender_output_acc: 0.9439 - image_quality_output_acc: 0.8724 - age_output_acc: 0.8688 - weight_output_acc: 0.8881 - bag_output_acc: 0.8928 - footwear_output_acc: 0.8998 - pose_output_acc: 0.9311 - emotion_output_acc: 0.9071Learning rate:  5e-07\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 5e-07.\n",
            "\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.2511 - gender_output_loss: 0.1410 - image_quality_output_loss: 0.3220 - age_output_loss: 0.3640 - weight_output_loss: 0.3066 - bag_output_loss: 0.2759 - footwear_output_loss: 0.2675 - pose_output_loss: 0.1854 - emotion_output_loss: 0.2796 - gender_output_acc: 0.9440 - image_quality_output_acc: 0.8722 - age_output_acc: 0.8690 - weight_output_acc: 0.8879 - bag_output_acc: 0.8928 - footwear_output_acc: 0.8997 - pose_output_acc: 0.9310 - emotion_output_acc: 0.9073 - val_loss: 11.0183 - val_gender_output_loss: 0.5052 - val_image_quality_output_loss: 1.4285 - val_age_output_loss: 2.3393 - val_weight_output_loss: 1.4965 - val_bag_output_loss: 1.2852 - val_footwear_output_loss: 1.1030 - val_pose_output_loss: 0.7376 - val_emotion_output_loss: 1.4214 - val_gender_output_acc: 0.8281 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5857 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 7.24353\n",
            "Epoch 79/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3258 - gender_output_loss: 0.1468 - image_quality_output_loss: 0.3314 - age_output_loss: 0.3840 - weight_output_loss: 0.3151 - bag_output_loss: 0.2826 - footwear_output_loss: 0.2760 - pose_output_loss: 0.1874 - emotion_output_loss: 0.2873 - gender_output_acc: 0.9386 - image_quality_output_acc: 0.8714 - age_output_acc: 0.8591 - weight_output_acc: 0.8839 - bag_output_acc: 0.8913 - footwear_output_acc: 0.8944 - pose_output_acc: 0.9301 - emotion_output_acc: 0.9036\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 5e-07.\n",
            "\n",
            "Epoch 79/100\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3257 - gender_output_loss: 0.1469 - image_quality_output_loss: 0.3313 - age_output_loss: 0.3841 - weight_output_loss: 0.3147 - bag_output_loss: 0.2825 - footwear_output_loss: 0.2756 - pose_output_loss: 0.1878 - emotion_output_loss: 0.2874 - gender_output_acc: 0.9385 - image_quality_output_acc: 0.8714 - age_output_acc: 0.8589 - weight_output_acc: 0.8841 - bag_output_acc: 0.8914 - footwear_output_acc: 0.8947 - pose_output_acc: 0.9299 - emotion_output_acc: 0.9036 - val_loss: 10.9939 - val_gender_output_loss: 0.5046 - val_image_quality_output_loss: 1.4180 - val_age_output_loss: 2.3308 - val_weight_output_loss: 1.4982 - val_bag_output_loss: 1.2850 - val_footwear_output_loss: 1.0994 - val_pose_output_loss: 0.7371 - val_emotion_output_loss: 1.4215 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.5650 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 7.24353\n",
            "Epoch 80/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3305 - gender_output_loss: 0.1482 - image_quality_output_loss: 0.3326 - age_output_loss: 0.3786 - weight_output_loss: 0.3103 - bag_output_loss: 0.2820 - footwear_output_loss: 0.2829 - pose_output_loss: 0.1840 - emotion_output_loss: 0.2984 - gender_output_acc: 0.9411 - image_quality_output_acc: 0.8707 - age_output_acc: 0.8609 - weight_output_acc: 0.8882 - bag_output_acc: 0.8931 - footwear_output_acc: 0.8908 - pose_output_acc: 0.9292 - emotion_output_acc: 0.8990 - val_loss: 11.0846 - val_gender_output_loss: 0.5094 - val_image_quality_output_loss: 1.4345 - val_age_output_loss: 2.3539 - val_weight_output_loss: 1.5078 - val_bag_output_loss: 1.2941 - val_footwear_output_loss: 1.1057 - val_pose_output_loss: 0.7455 - val_emotion_output_loss: 1.4274 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3609 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 7.24353\n",
            "Epoch 81/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3247 - gender_output_loss: 0.1470 - image_quality_output_loss: 0.3306 - age_output_loss: 0.3779 - weight_output_loss: 0.3091 - bag_output_loss: 0.2897 - footwear_output_loss: 0.2720 - pose_output_loss: 0.1926 - emotion_output_loss: 0.2924 - gender_output_acc: 0.9399 - image_quality_output_acc: 0.8696 - age_output_acc: 0.8595 - weight_output_acc: 0.8872 - bag_output_acc: 0.8868 - footwear_output_acc: 0.8982 - pose_output_acc: 0.9253 - emotion_output_acc: 0.9009 - val_loss: 11.0311 - val_gender_output_loss: 0.5068 - val_image_quality_output_loss: 1.4230 - val_age_output_loss: 2.3428 - val_weight_output_loss: 1.4979 - val_bag_output_loss: 1.2874 - val_footwear_output_loss: 1.1008 - val_pose_output_loss: 0.7444 - val_emotion_output_loss: 1.4251 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3589 - val_weight_output_acc: 0.5600 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.7767 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 7.24353\n",
            "Epoch 82/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.2930 - gender_output_loss: 0.1429 - image_quality_output_loss: 0.3360 - age_output_loss: 0.3724 - weight_output_loss: 0.3121 - bag_output_loss: 0.2743 - footwear_output_loss: 0.2682 - pose_output_loss: 0.1823 - emotion_output_loss: 0.2930 - gender_output_acc: 0.9414 - image_quality_output_acc: 0.8663 - age_output_acc: 0.8644 - weight_output_acc: 0.8860 - bag_output_acc: 0.8975 - footwear_output_acc: 0.9013 - pose_output_acc: 0.9327 - emotion_output_acc: 0.8984 - val_loss: 11.0560 - val_gender_output_loss: 0.5072 - val_image_quality_output_loss: 1.4253 - val_age_output_loss: 2.3571 - val_weight_output_loss: 1.4987 - val_bag_output_loss: 1.2905 - val_footwear_output_loss: 1.1015 - val_pose_output_loss: 0.7402 - val_emotion_output_loss: 1.4284 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 7.24353\n",
            "Epoch 83/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3161 - gender_output_loss: 0.1515 - image_quality_output_loss: 0.3260 - age_output_loss: 0.3773 - weight_output_loss: 0.3131 - bag_output_loss: 0.2836 - footwear_output_loss: 0.2717 - pose_output_loss: 0.1882 - emotion_output_loss: 0.2916 - gender_output_acc: 0.9381 - image_quality_output_acc: 0.8712 - age_output_acc: 0.8614 - weight_output_acc: 0.8848 - bag_output_acc: 0.8910 - footwear_output_acc: 0.8943 - pose_output_acc: 0.9300 - emotion_output_acc: 0.9021 - val_loss: 11.0679 - val_gender_output_loss: 0.5072 - val_image_quality_output_loss: 1.4323 - val_age_output_loss: 2.3515 - val_weight_output_loss: 1.5020 - val_bag_output_loss: 1.2904 - val_footwear_output_loss: 1.1076 - val_pose_output_loss: 0.7466 - val_emotion_output_loss: 1.4248 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3523 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7777 - val_emotion_output_acc: 0.6366\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 7.24353\n",
            "Epoch 84/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3405 - gender_output_loss: 0.1484 - image_quality_output_loss: 0.3354 - age_output_loss: 0.3834 - weight_output_loss: 0.3206 - bag_output_loss: 0.2907 - footwear_output_loss: 0.2753 - pose_output_loss: 0.1869 - emotion_output_loss: 0.2846 - gender_output_acc: 0.9373 - image_quality_output_acc: 0.8700 - age_output_acc: 0.8598 - weight_output_acc: 0.8822 - bag_output_acc: 0.8882 - footwear_output_acc: 0.8949 - pose_output_acc: 0.9300 - emotion_output_acc: 0.9034\n",
            "Epoch 00083: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3412 - gender_output_loss: 0.1485 - image_quality_output_loss: 0.3357 - age_output_loss: 0.3833 - weight_output_loss: 0.3206 - bag_output_loss: 0.2912 - footwear_output_loss: 0.2752 - pose_output_loss: 0.1873 - emotion_output_loss: 0.2844 - gender_output_acc: 0.9373 - image_quality_output_acc: 0.8699 - age_output_acc: 0.8597 - weight_output_acc: 0.8822 - bag_output_acc: 0.8879 - footwear_output_acc: 0.8949 - pose_output_acc: 0.9297 - emotion_output_acc: 0.9035 - val_loss: 11.0419 - val_gender_output_loss: 0.5065 - val_image_quality_output_loss: 1.4326 - val_age_output_loss: 2.3455 - val_weight_output_loss: 1.4993 - val_bag_output_loss: 1.2817 - val_footwear_output_loss: 1.1047 - val_pose_output_loss: 0.7416 - val_emotion_output_loss: 1.4263 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5544 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6447 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6411\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 7.24353\n",
            "Epoch 85/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3054 - gender_output_loss: 0.1439 - image_quality_output_loss: 0.3268 - age_output_loss: 0.3797 - weight_output_loss: 0.3089 - bag_output_loss: 0.2831 - footwear_output_loss: 0.2731 - pose_output_loss: 0.1873 - emotion_output_loss: 0.2887 - gender_output_acc: 0.9416 - image_quality_output_acc: 0.8726 - age_output_acc: 0.8606 - weight_output_acc: 0.8880 - bag_output_acc: 0.8941 - footwear_output_acc: 0.8963 - pose_output_acc: 0.9284 - emotion_output_acc: 0.9027\n",
            "Epoch 00084: val_loss did not improve from 7.24353\n",
            "Learning rate:  5e-07\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3052 - gender_output_loss: 0.1436 - image_quality_output_loss: 0.3272 - age_output_loss: 0.3797 - weight_output_loss: 0.3084 - bag_output_loss: 0.2831 - footwear_output_loss: 0.2731 - pose_output_loss: 0.1874 - emotion_output_loss: 0.2889 - gender_output_acc: 0.9417 - image_quality_output_acc: 0.8722 - age_output_acc: 0.8608 - weight_output_acc: 0.8882 - bag_output_acc: 0.8940 - footwear_output_acc: 0.8964 - pose_output_acc: 0.9283 - emotion_output_acc: 0.9028 - val_loss: 10.9846 - val_gender_output_loss: 0.5026 - val_image_quality_output_loss: 1.4217 - val_age_output_loss: 2.3325 - val_weight_output_loss: 1.4920 - val_bag_output_loss: 1.2827 - val_footwear_output_loss: 1.0960 - val_pose_output_loss: 0.7389 - val_emotion_output_loss: 1.4185 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.5580 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7767 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 7.24353\n",
            "Epoch 86/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3171 - gender_output_loss: 0.1449 - image_quality_output_loss: 0.3323 - age_output_loss: 0.3809 - weight_output_loss: 0.3138 - bag_output_loss: 0.2790 - footwear_output_loss: 0.2722 - pose_output_loss: 0.1899 - emotion_output_loss: 0.2899 - gender_output_acc: 0.9401 - image_quality_output_acc: 0.8645 - age_output_acc: 0.8565 - weight_output_acc: 0.8869 - bag_output_acc: 0.8945 - footwear_output_acc: 0.8981 - pose_output_acc: 0.9279 - emotion_output_acc: 0.9019\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3163 - gender_output_loss: 0.1447 - image_quality_output_loss: 0.3321 - age_output_loss: 0.3808 - weight_output_loss: 0.3138 - bag_output_loss: 0.2787 - footwear_output_loss: 0.2720 - pose_output_loss: 0.1897 - emotion_output_loss: 0.2902 - gender_output_acc: 0.9401 - image_quality_output_acc: 0.8647 - age_output_acc: 0.8562 - weight_output_acc: 0.8870 - bag_output_acc: 0.8947 - footwear_output_acc: 0.8983 - pose_output_acc: 0.9280 - emotion_output_acc: 0.9016 - val_loss: 11.1273 - val_gender_output_loss: 0.5125 - val_image_quality_output_loss: 1.4372 - val_age_output_loss: 2.3607 - val_weight_output_loss: 1.5118 - val_bag_output_loss: 1.2963 - val_footwear_output_loss: 1.1118 - val_pose_output_loss: 0.7460 - val_emotion_output_loss: 1.4428 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3543 - val_weight_output_acc: 0.5585 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7772 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 7.24353\n",
            "Epoch 87/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3336 - gender_output_loss: 0.1533 - image_quality_output_loss: 0.3376 - age_output_loss: 0.3849 - weight_output_loss: 0.3176 - bag_output_loss: 0.2772 - footwear_output_loss: 0.2730 - pose_output_loss: 0.1850 - emotion_output_loss: 0.2895 - gender_output_acc: 0.9376 - image_quality_output_acc: 0.8646 - age_output_acc: 0.8558 - weight_output_acc: 0.8840 - bag_output_acc: 0.8938 - footwear_output_acc: 0.8936 - pose_output_acc: 0.9289 - emotion_output_acc: 0.9030 - val_loss: 11.0630 - val_gender_output_loss: 0.5067 - val_image_quality_output_loss: 1.4275 - val_age_output_loss: 2.3524 - val_weight_output_loss: 1.5034 - val_bag_output_loss: 1.2958 - val_footwear_output_loss: 1.1032 - val_pose_output_loss: 0.7427 - val_emotion_output_loss: 1.4254 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3569 - val_weight_output_acc: 0.5630 - val_bag_output_acc: 0.5832 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6361\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 7.24353\n",
            "Epoch 88/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3017 - gender_output_loss: 0.1409 - image_quality_output_loss: 0.3337 - age_output_loss: 0.3836 - weight_output_loss: 0.3023 - bag_output_loss: 0.2826 - footwear_output_loss: 0.2754 - pose_output_loss: 0.1799 - emotion_output_loss: 0.2882 - gender_output_acc: 0.9436 - image_quality_output_acc: 0.8689 - age_output_acc: 0.8577 - weight_output_acc: 0.8888 - bag_output_acc: 0.8927 - footwear_output_acc: 0.8924 - pose_output_acc: 0.9312 - emotion_output_acc: 0.9030 - val_loss: 11.0639 - val_gender_output_loss: 0.5089 - val_image_quality_output_loss: 1.4321 - val_age_output_loss: 2.3493 - val_weight_output_loss: 1.5012 - val_bag_output_loss: 1.2906 - val_footwear_output_loss: 1.1049 - val_pose_output_loss: 0.7416 - val_emotion_output_loss: 1.4307 - val_gender_output_acc: 0.8276 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5615 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 7.24353\n",
            "Epoch 89/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3099 - gender_output_loss: 0.1403 - image_quality_output_loss: 0.3359 - age_output_loss: 0.3802 - weight_output_loss: 0.3095 - bag_output_loss: 0.2790 - footwear_output_loss: 0.2715 - pose_output_loss: 0.1847 - emotion_output_loss: 0.2947 - gender_output_acc: 0.9429 - image_quality_output_acc: 0.8680 - age_output_acc: 0.8587 - weight_output_acc: 0.8857 - bag_output_acc: 0.8941 - footwear_output_acc: 0.8985 - pose_output_acc: 0.9307 - emotion_output_acc: 0.9000 5e-07\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3110 - gender_output_loss: 0.1404 - image_quality_output_loss: 0.3365 - age_output_loss: 0.3801 - weight_output_loss: 0.3095 - bag_output_loss: 0.2790 - footwear_output_loss: 0.2716 - pose_output_loss: 0.1848 - emotion_output_loss: 0.2951 - gender_output_acc: 0.9428 - image_quality_output_acc: 0.8677 - age_output_acc: 0.8588 - weight_output_acc: 0.8859 - bag_output_acc: 0.8940 - footwear_output_acc: 0.8984 - pose_output_acc: 0.9306 - emotion_output_acc: 0.8999 - val_loss: 11.1001 - val_gender_output_loss: 0.5102 - val_image_quality_output_loss: 1.4426 - val_age_output_loss: 2.3583 - val_weight_output_loss: 1.5020 - val_bag_output_loss: 1.2962 - val_footwear_output_loss: 1.1100 - val_pose_output_loss: 0.7410 - val_emotion_output_loss: 1.4324 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.7787 - val_emotion_output_acc: 0.6396\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 7.24353\n",
            "Epoch 90/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3422 - gender_output_loss: 0.1528 - image_quality_output_loss: 0.3355 - age_output_loss: 0.3811 - weight_output_loss: 0.3163 - bag_output_loss: 0.2873 - footwear_output_loss: 0.2717 - pose_output_loss: 0.1846 - emotion_output_loss: 0.2987 - gender_output_acc: 0.9374 - image_quality_output_acc: 0.8653 - age_output_acc: 0.8592 - weight_output_acc: 0.8836 - bag_output_acc: 0.8868 - footwear_output_acc: 0.8975 - pose_output_acc: 0.9307 - emotion_output_acc: 0.8994 - val_loss: 10.9999 - val_gender_output_loss: 0.5045 - val_image_quality_output_loss: 1.4252 - val_age_output_loss: 2.3390 - val_weight_output_loss: 1.4953 - val_bag_output_loss: 1.2818 - val_footwear_output_loss: 1.0960 - val_pose_output_loss: 0.7409 - val_emotion_output_loss: 1.4154 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3533 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6497 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 7.24353\n",
            "Epoch 91/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.2878 - gender_output_loss: 0.1474 - image_quality_output_loss: 0.3269 - age_output_loss: 0.3735 - weight_output_loss: 0.3030 - bag_output_loss: 0.2791 - footwear_output_loss: 0.2696 - pose_output_loss: 0.1827 - emotion_output_loss: 0.2935 - gender_output_acc: 0.9407 - image_quality_output_acc: 0.8747 - age_output_acc: 0.8639 - weight_output_acc: 0.8891 - bag_output_acc: 0.8944 - footwear_output_acc: 0.8969 - pose_output_acc: 0.9321 - emotion_output_acc: 0.8991 - val_loss: 11.0088 - val_gender_output_loss: 0.5066 - val_image_quality_output_loss: 1.4253 - val_age_output_loss: 2.3366 - val_weight_output_loss: 1.4931 - val_bag_output_loss: 1.2855 - val_footwear_output_loss: 1.0987 - val_pose_output_loss: 0.7386 - val_emotion_output_loss: 1.4234 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3564 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6421 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 7.24353\n",
            "Epoch 92/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 143ms/step - loss: 2.3457 - gender_output_loss: 0.1498 - image_quality_output_loss: 0.3351 - age_output_loss: 0.3801 - weight_output_loss: 0.3145 - bag_output_loss: 0.2816 - footwear_output_loss: 0.2829 - pose_output_loss: 0.1930 - emotion_output_loss: 0.2946 - gender_output_acc: 0.9391 - image_quality_output_acc: 0.8663 - age_output_acc: 0.8596 - weight_output_acc: 0.8867 - bag_output_acc: 0.8899 - footwear_output_acc: 0.8935 - pose_output_acc: 0.9274 - emotion_output_acc: 0.8984 - val_loss: 11.1501 - val_gender_output_loss: 0.5129 - val_image_quality_output_loss: 1.4440 - val_age_output_loss: 2.3647 - val_weight_output_loss: 1.5150 - val_bag_output_loss: 1.3020 - val_footwear_output_loss: 1.1124 - val_pose_output_loss: 0.7455 - val_emotion_output_loss: 1.4442 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5227 - val_age_output_acc: 0.3584 - val_weight_output_acc: 0.5595 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.7767 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 7.24353\n",
            "Epoch 93/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.3680 - gender_output_loss: 0.1581 - image_quality_output_loss: 0.3365 - age_output_loss: 0.3823 - weight_output_loss: 0.3207 - bag_output_loss: 0.2825 - footwear_output_loss: 0.2792 - pose_output_loss: 0.1901 - emotion_output_loss: 0.3038 - gender_output_acc: 0.9360 - image_quality_output_acc: 0.8666 - age_output_acc: 0.8607 - weight_output_acc: 0.8856 - bag_output_acc: 0.8916 - footwear_output_acc: 0.8919 - pose_output_acc: 0.9291 - emotion_output_acc: 0.8965 - val_loss: 11.0527 - val_gender_output_loss: 0.5053 - val_image_quality_output_loss: 1.4328 - val_age_output_loss: 2.3446 - val_weight_output_loss: 1.5043 - val_bag_output_loss: 1.2905 - val_footwear_output_loss: 1.1026 - val_pose_output_loss: 0.7408 - val_emotion_output_loss: 1.4284 - val_gender_output_acc: 0.8261 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3589 - val_weight_output_acc: 0.5544 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6371\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 7.24353\n",
            "Epoch 94/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.2669 - gender_output_loss: 0.1443 - image_quality_output_loss: 0.3301 - age_output_loss: 0.3585 - weight_output_loss: 0.3091 - bag_output_loss: 0.2651 - footwear_output_loss: 0.2766 - pose_output_loss: 0.1816 - emotion_output_loss: 0.2940 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.8687 - age_output_acc: 0.8679 - weight_output_acc: 0.8880 - bag_output_acc: 0.8978 - footwear_output_acc: 0.8948 - pose_output_acc: 0.9297 - emotion_output_acc: 0.8959Epoch 94/100\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.2680 - gender_output_loss: 0.1444 - image_quality_output_loss: 0.3301 - age_output_loss: 0.3584 - weight_output_loss: 0.3091 - bag_output_loss: 0.2653 - footwear_output_loss: 0.2770 - pose_output_loss: 0.1818 - emotion_output_loss: 0.2943 - gender_output_acc: 0.9404 - image_quality_output_acc: 0.8689 - age_output_acc: 0.8681 - weight_output_acc: 0.8878 - bag_output_acc: 0.8977 - footwear_output_acc: 0.8947 - pose_output_acc: 0.9295 - emotion_output_acc: 0.8957 - val_loss: 10.9969 - val_gender_output_loss: 0.5036 - val_image_quality_output_loss: 1.4281 - val_age_output_loss: 2.3278 - val_weight_output_loss: 1.4961 - val_bag_output_loss: 1.2849 - val_footwear_output_loss: 1.0986 - val_pose_output_loss: 0.7365 - val_emotion_output_loss: 1.4229 - val_gender_output_acc: 0.8276 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5595 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6477 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6401\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 7.24353\n",
            "Epoch 95/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3149 - gender_output_loss: 0.1433 - image_quality_output_loss: 0.3353 - age_output_loss: 0.3785 - weight_output_loss: 0.3075 - bag_output_loss: 0.2833 - footwear_output_loss: 0.2764 - pose_output_loss: 0.1811 - emotion_output_loss: 0.2959 - gender_output_acc: 0.9408 - image_quality_output_acc: 0.8641 - age_output_acc: 0.8599 - weight_output_acc: 0.8859 - bag_output_acc: 0.8872 - footwear_output_acc: 0.8932 - pose_output_acc: 0.9320 - emotion_output_acc: 0.8984 - val_loss: 10.9660 - val_gender_output_loss: 0.5037 - val_image_quality_output_loss: 1.4253 - val_age_output_loss: 2.3233 - val_weight_output_loss: 1.4899 - val_bag_output_loss: 1.2804 - val_footwear_output_loss: 1.0929 - val_pose_output_loss: 0.7332 - val_emotion_output_loss: 1.4202 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5620 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6462 - val_pose_output_acc: 0.7782 - val_emotion_output_acc: 0.6381\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 7.24353\n",
            "Epoch 96/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.2061 - gender_output_loss: 0.1428 - image_quality_output_loss: 0.3181 - age_output_loss: 0.3583 - weight_output_loss: 0.3018 - bag_output_loss: 0.2650 - footwear_output_loss: 0.2659 - pose_output_loss: 0.1744 - emotion_output_loss: 0.2723 - gender_output_acc: 0.9435 - image_quality_output_acc: 0.8741 - age_output_acc: 0.8701 - weight_output_acc: 0.8898 - bag_output_acc: 0.8973 - footwear_output_acc: 0.8986 - pose_output_acc: 0.9344 - emotion_output_acc: 0.9126 - val_loss: 10.9864 - val_gender_output_loss: 0.5025 - val_image_quality_output_loss: 1.4262 - val_age_output_loss: 2.3311 - val_weight_output_loss: 1.4905 - val_bag_output_loss: 1.2790 - val_footwear_output_loss: 1.0966 - val_pose_output_loss: 0.7401 - val_emotion_output_loss: 1.4211 - val_gender_output_acc: 0.8256 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5559 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7762 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 7.24353\n",
            "Epoch 97/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 145ms/step - loss: 2.3131 - gender_output_loss: 0.1438 - image_quality_output_loss: 0.3333 - age_output_loss: 0.3758 - weight_output_loss: 0.3124 - bag_output_loss: 0.2805 - footwear_output_loss: 0.2751 - pose_output_loss: 0.1925 - emotion_output_loss: 0.2868 - gender_output_acc: 0.9406 - image_quality_output_acc: 0.8687 - age_output_acc: 0.8627 - weight_output_acc: 0.8850 - bag_output_acc: 0.8928 - footwear_output_acc: 0.8965 - pose_output_acc: 0.9267 - emotion_output_acc: 0.9056 - val_loss: 11.0670 - val_gender_output_loss: 0.5072 - val_image_quality_output_loss: 1.4330 - val_age_output_loss: 2.3501 - val_weight_output_loss: 1.5046 - val_bag_output_loss: 1.2913 - val_footwear_output_loss: 1.1020 - val_pose_output_loss: 0.7420 - val_emotion_output_loss: 1.4317 - val_gender_output_acc: 0.8276 - val_image_quality_output_acc: 0.5202 - val_age_output_acc: 0.3558 - val_weight_output_acc: 0.5615 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.7812 - val_emotion_output_acc: 0.6346\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 7.24353\n",
            "Epoch 98/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 5e-07.\n",
            "360/360 [==============================] - 52s 143ms/step - loss: 2.3247 - gender_output_loss: 0.1455 - image_quality_output_loss: 0.3257 - age_output_loss: 0.3876 - weight_output_loss: 0.3169 - bag_output_loss: 0.2816 - footwear_output_loss: 0.2756 - pose_output_loss: 0.1795 - emotion_output_loss: 0.2961 - gender_output_acc: 0.9417 - image_quality_output_acc: 0.8707 - age_output_acc: 0.8563 - weight_output_acc: 0.8841 - bag_output_acc: 0.8925 - footwear_output_acc: 0.8964 - pose_output_acc: 0.9343 - emotion_output_acc: 0.9010 - val_loss: 11.0420 - val_gender_output_loss: 0.5053 - val_image_quality_output_loss: 1.4293 - val_age_output_loss: 2.3467 - val_weight_output_loss: 1.4979 - val_bag_output_loss: 1.2898 - val_footwear_output_loss: 1.1019 - val_pose_output_loss: 0.7425 - val_emotion_output_loss: 1.4247 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5262 - val_age_output_acc: 0.3538 - val_weight_output_acc: 0.5590 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.7818 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 7.24353\n",
            "Epoch 99/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.2696 - gender_output_loss: 0.1435 - image_quality_output_loss: 0.3246 - age_output_loss: 0.3661 - weight_output_loss: 0.3073 - bag_output_loss: 0.2804 - footwear_output_loss: 0.2759 - pose_output_loss: 0.1816 - emotion_output_loss: 0.2803 - gender_output_acc: 0.9402 - image_quality_output_acc: 0.8759 - age_output_acc: 0.8656 - weight_output_acc: 0.8884 - bag_output_acc: 0.8943 - footwear_output_acc: 0.8934 - pose_output_acc: 0.9319 - emotion_output_acc: 0.9062\n",
            "360/360 [==============================] - 51s 142ms/step - loss: 2.2692 - gender_output_loss: 0.1436 - image_quality_output_loss: 0.3243 - age_output_loss: 0.3663 - weight_output_loss: 0.3069 - bag_output_loss: 0.2804 - footwear_output_loss: 0.2759 - pose_output_loss: 0.1818 - emotion_output_loss: 0.2801 - gender_output_acc: 0.9402 - image_quality_output_acc: 0.8760 - age_output_acc: 0.8655 - weight_output_acc: 0.8886 - bag_output_acc: 0.8944 - footwear_output_acc: 0.8934 - pose_output_acc: 0.9318 - emotion_output_acc: 0.9063 - val_loss: 11.0289 - val_gender_output_loss: 0.5063 - val_image_quality_output_loss: 1.4282 - val_age_output_loss: 2.3395 - val_weight_output_loss: 1.4962 - val_bag_output_loss: 1.2897 - val_footwear_output_loss: 1.1025 - val_pose_output_loss: 0.7405 - val_emotion_output_loss: 1.4243 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5610 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6472 - val_pose_output_acc: 0.7797 - val_emotion_output_acc: 0.6386\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 7.24353\n",
            "Epoch 100/100\n",
            "Learning rate:  5e-07\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 5e-07.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 2.3096 - gender_output_loss: 0.1464 - image_quality_output_loss: 0.3212 - age_output_loss: 0.3775 - weight_output_loss: 0.3141 - bag_output_loss: 0.2737 - footwear_output_loss: 0.2728 - pose_output_loss: 0.1941 - emotion_output_loss: 0.2965 - gender_output_acc: 0.9405 - image_quality_output_acc: 0.8740 - age_output_acc: 0.8605 - weight_output_acc: 0.8868 - bag_output_acc: 0.8936 - footwear_output_acc: 0.8953 - pose_output_acc: 0.9281 - emotion_output_acc: 0.9005Learning rate:  5e-07\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 7.24353\n",
            "360/360 [==============================] - 52s 144ms/step - loss: 2.3073 - gender_output_loss: 0.1465 - image_quality_output_loss: 0.3210 - age_output_loss: 0.3770 - weight_output_loss: 0.3139 - bag_output_loss: 0.2735 - footwear_output_loss: 0.2724 - pose_output_loss: 0.1938 - emotion_output_loss: 0.2961 - gender_output_acc: 0.9406 - image_quality_output_acc: 0.8742 - age_output_acc: 0.8606 - weight_output_acc: 0.8869 - bag_output_acc: 0.8938 - footwear_output_acc: 0.8955 - pose_output_acc: 0.9282 - emotion_output_acc: 0.9007 - val_loss: 11.0530 - val_gender_output_loss: 0.5063 - val_image_quality_output_loss: 1.4302 - val_age_output_loss: 2.3503 - val_weight_output_loss: 1.5013 - val_bag_output_loss: 1.2915 - val_footwear_output_loss: 1.1031 - val_pose_output_loss: 0.7409 - val_emotion_output_loss: 1.4243 - val_gender_output_acc: 0.8271 - val_image_quality_output_acc: 0.5217 - val_age_output_acc: 0.3574 - val_weight_output_acc: 0.5570 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6457 - val_pose_output_acc: 0.7792 - val_emotion_output_acc: 0.6391\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 7.24353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8c48b630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKStBcwhBKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5d593bc-2326-4d57-88f4-2669a6f83b44"
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 5s 174ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw0ayvhWFlzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "844981f2-e83e-4db0-e3ef-7b1e6ba3441f"
      },
      "source": [
        "results"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.052993097612935,\n",
              " 0.5063016563653946,\n",
              " 1.4301760408186144,\n",
              " 2.350307191571882,\n",
              " 1.5013189777251212,\n",
              " 1.2914905125094998,\n",
              " 1.1030988693237305,\n",
              " 0.7409410813162404,\n",
              " 1.4242667478899802,\n",
              " 0.827116935483871,\n",
              " 0.5216733870967742,\n",
              " 0.35735887096774194,\n",
              " 0.5569556451612904,\n",
              " 0.5871975806451613,\n",
              " 0.6456653225806451,\n",
              " 0.7792338709677419,\n",
              " 0.6391129032258065]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oQLhLevhok-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}